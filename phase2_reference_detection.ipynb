{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Phase 2: Visual Reference Detection (English)\n",
    "## Detects when speaker references visual elements\n",
    "\n",
    "This notebook:\n",
    "- Loads Phase 1 transcript (from Whisper)\n",
    "- Detects visual references using regex + BERT\n",
    "- Outputs timestamped reference points\n",
    "\n",
    "**Uses:** Regex patterns + sentence-transformers for semantic matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 1: Install Dependencies\n",
    "# ====================================================================\n",
    "print(\"ðŸ“¦ Installing dependencies...\")\n",
    "!pip install -q sentence-transformers torch numpy\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 2: Import Libraries\n",
    "# ====================================================================\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gc\n",
    "\n",
    "print(f\"ðŸ–¥ï¸  Device: {'CUDA (GPU)' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 3: Configuration\n",
    "# ====================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Input from Phase 1 (change this to your file)\n",
    "    \"phase1_file\": \"/kaggle/working/output/transcript_XXXXX.json\",  # âš ï¸ UPDATE THIS\n",
    "    \n",
    "    # Reference detection settings\n",
    "    \"use_bert\": True,  # Use semantic matching (recommended)\n",
    "    \"bert_model\": \"all-MiniLM-L6-v2\",  # Fast, lightweight model\n",
    "    \"similarity_threshold\": 0.45,  # Lower = more sensitive (0.3-0.6)\n",
    "    \n",
    "    # English reference phrases to detect\n",
    "    \"reference_phrases\": [\n",
    "        # Direct visual references\n",
    "        \"look at this\",\n",
    "        \"as you can see\",\n",
    "        \"shown here\",\n",
    "        \"in this diagram\",\n",
    "        \"this figure\",\n",
    "        \"this image\",\n",
    "        \"this graph\",\n",
    "        \"this chart\",\n",
    "        \"this table\",\n",
    "        \"this equation\",\n",
    "        \"this formula\",\n",
    "        \n",
    "        # Pointing/showing\n",
    "        \"here you can see\",\n",
    "        \"if you look at\",\n",
    "        \"take a look at\",\n",
    "        \"let me show you\",\n",
    "        \"I'll show you\",\n",
    "        \"notice here\",\n",
    "        \"observe that\",\n",
    "        \n",
    "        # Diagram/visual elements\n",
    "        \"on the screen\",\n",
    "        \"on the board\",\n",
    "        \"in the picture\",\n",
    "        \"in the video\",\n",
    "        \"this example\",\n",
    "        \"this visualization\",\n",
    "        \n",
    "        # Positional\n",
    "        \"on the left\",\n",
    "        \"on the right\",\n",
    "        \"at the top\",\n",
    "        \"at the bottom\",\n",
    "        \"in the middle\",\n",
    "        \"over here\",\n",
    "    ],\n",
    "    \n",
    "    \"output_dir\": \"/kaggle/working/output\"\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"   BERT: {'Enabled' if CONFIG['use_bert'] else 'Disabled'}\")\n",
    "print(f\"   Similarity threshold: {CONFIG['similarity_threshold']}\")\n",
    "print(f\"   Reference phrases: {len(CONFIG['reference_phrases'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_phase1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 4: Load Phase 1 Output\n",
    "# ====================================================================\n",
    "\n",
    "print(f\"ðŸ“‚ Loading Phase 1 transcript from: {CONFIG['phase1_file']}\")\n",
    "\n",
    "with open(CONFIG['phase1_file'], 'r', encoding='utf-8') as f:\n",
    "    phase1_data = json.load(f)\n",
    "\n",
    "utterances = phase1_data['utterances']\n",
    "video_id = phase1_data['video_id']\n",
    "\n",
    "print(f\"âœ… Loaded transcript:\")\n",
    "print(f\"   Video ID: {video_id}\")\n",
    "print(f\"   Utterances: {len(utterances)}\")\n",
    "print(f\"   Duration: {phase1_data['duration_ms']/1000:.1f}s\")\n",
    "print(f\"\\nðŸ“ First utterance: {utterances[0]['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detector_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 5: Reference Detector Class\n",
    "# ====================================================================\n",
    "\n",
    "class EnglishReferenceDetector:\n",
    "    \"\"\"Detect visual references in English transcripts.\"\"\"\n",
    "    \n",
    "    def __init__(self, phrases: List[str], model_name: str = None,\n",
    "                 use_bert: bool = True, threshold: float = 0.45):\n",
    "        \"\"\"\n",
    "        Initialize detector.\n",
    "        \n",
    "        Args:\n",
    "            phrases: List of reference phrases to detect\n",
    "            model_name: Sentence transformer model\n",
    "            use_bert: Whether to use semantic matching\n",
    "            threshold: Similarity threshold (0.0-1.0)\n",
    "        \"\"\"\n",
    "        self.phrases = phrases\n",
    "        self.use_bert = use_bert\n",
    "        self.threshold = threshold\n",
    "        self.model = None\n",
    "        \n",
    "        # Compile regex patterns (case-insensitive)\n",
    "        self.patterns = [\n",
    "            re.compile(r'\\b' + re.escape(phrase) + r'\\b', re.IGNORECASE)\n",
    "            for phrase in phrases\n",
    "        ]\n",
    "        \n",
    "        # Load semantic model if requested\n",
    "        if use_bert and model_name:\n",
    "            print(f\"ðŸ¤– Loading semantic model: {model_name}\")\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.model = SentenceTransformer(model_name, device=device)\n",
    "            print(f\"âœ… Model loaded on {device}\")\n",
    "    \n",
    "    def detect_with_regex(self, utterances: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Detect references using regex pattern matching.\"\"\"\n",
    "        print(\"ðŸ” Detecting with regex patterns...\")\n",
    "        references = []\n",
    "        reference_id = 1\n",
    "        \n",
    "        for utt_idx, utterance in enumerate(utterances):\n",
    "            text = utterance['text']\n",
    "            \n",
    "            for phrase, pattern in zip(self.phrases, self.patterns):\n",
    "                match = pattern.search(text)\n",
    "                \n",
    "                if match:\n",
    "                    references.append({\n",
    "                        \"reference_id\": f\"REF_{reference_id:03d}\",\n",
    "                        \"text\": text,\n",
    "                        \"timestamp_ms\": utterance['start_ms'],\n",
    "                        \"end_ms\": utterance['end_ms'],\n",
    "                        \"matched_phrase\": phrase,\n",
    "                        \"reference_type\": self._classify_type(phrase),\n",
    "                        \"detection_method\": \"regex\",\n",
    "                        \"confidence\": 0.95,\n",
    "                        \"context_before\": self._get_context(utterances, utt_idx, -1),\n",
    "                        \"context_after\": self._get_context(utterances, utt_idx, 1)\n",
    "                    })\n",
    "                    reference_id += 1\n",
    "                    break  # Only count once per utterance\n",
    "        \n",
    "        print(f\"   Found {len(references)} regex matches\")\n",
    "        return references\n",
    "    \n",
    "    def detect_with_semantic(self, utterances: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Detect references using semantic similarity.\"\"\"\n",
    "        if not self.model:\n",
    "            print(\"âš ï¸  Semantic model not loaded, skipping\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"ðŸ§  Detecting with semantic matching (threshold={self.threshold})...\")\n",
    "        references = []\n",
    "        reference_id = 1000  # Different ID range for semantic\n",
    "        \n",
    "        # Encode reference phrases\n",
    "        print(\"   Encoding reference phrases...\")\n",
    "        reference_embeddings = self.model.encode(\n",
    "            self.phrases,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Process in batches to save memory\n",
    "        batch_size = 32\n",
    "        print(f\"   Processing {len(utterances)} utterances in batches...\")\n",
    "        \n",
    "        for batch_start in range(0, len(utterances), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(utterances))\n",
    "            batch = utterances[batch_start:batch_end]\n",
    "            \n",
    "            # Encode batch\n",
    "            texts = [u['text'] for u in batch]\n",
    "            text_embeddings = self.model.encode(\n",
    "                texts,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = torch.mm(text_embeddings, reference_embeddings.T)\n",
    "            max_sims, _ = similarities.max(dim=1)\n",
    "            \n",
    "            # Find matches\n",
    "            for i, (utterance, sim) in enumerate(zip(batch, max_sims)):\n",
    "                if sim.item() >= self.threshold:\n",
    "                    utt_idx = batch_start + i\n",
    "                    references.append({\n",
    "                        \"reference_id\": f\"SEM_{reference_id:03d}\",\n",
    "                        \"text\": utterance['text'],\n",
    "                        \"timestamp_ms\": utterance['start_ms'],\n",
    "                        \"end_ms\": utterance['end_ms'],\n",
    "                        \"matched_phrase\": \"semantic_match\",\n",
    "                        \"reference_type\": \"contextual\",\n",
    "                        \"detection_method\": \"semantic\",\n",
    "                        \"confidence\": float(sim.item()),\n",
    "                        \"context_before\": self._get_context(utterances, utt_idx, -1),\n",
    "                        \"context_after\": self._get_context(utterances, utt_idx, 1)\n",
    "                    })\n",
    "                    reference_id += 1\n",
    "        \n",
    "        # Clean up\n",
    "        del reference_embeddings, text_embeddings\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"   Found {len(references)} semantic matches\")\n",
    "        return references\n",
    "    \n",
    "    def _classify_type(self, phrase: str) -> str:\n",
    "        \"\"\"Classify reference type based on phrase.\"\"\"\n",
    "        phrase_lower = phrase.lower()\n",
    "        \n",
    "        if 'diagram' in phrase_lower or 'figure' in phrase_lower:\n",
    "            return 'diagram'\n",
    "        elif 'equation' in phrase_lower or 'formula' in phrase_lower:\n",
    "            return 'equation'\n",
    "        elif 'graph' in phrase_lower or 'chart' in phrase_lower:\n",
    "            return 'graph'\n",
    "        elif 'table' in phrase_lower:\n",
    "            return 'table'\n",
    "        elif 'screen' in phrase_lower or 'board' in phrase_lower:\n",
    "            return 'screen'\n",
    "        else:\n",
    "            return 'visual'\n",
    "    \n",
    "    def _get_context(self, utterances: List[Dict], index: int, offset: int) -> str:\n",
    "        \"\"\"Get context utterance.\"\"\"\n",
    "        target_idx = index + offset\n",
    "        if 0 <= target_idx < len(utterances):\n",
    "            return utterances[target_idx]['text']\n",
    "        return \"\"\n",
    "\n",
    "print(\"âœ… Detector class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 6: Run Detection\n",
    "# ====================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ STARTING REFERENCE DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize detector\n",
    "detector = EnglishReferenceDetector(\n",
    "    phrases=CONFIG['reference_phrases'],\n",
    "    model_name=CONFIG['bert_model'] if CONFIG['use_bert'] else None,\n",
    "    use_bert=CONFIG['use_bert'],\n",
    "    threshold=CONFIG['similarity_threshold']\n",
    ")\n",
    "\n",
    "# Step 1: Regex detection (fast, high precision)\n",
    "print(\"\\n[1/3] Regex pattern matching...\")\n",
    "regex_refs = detector.detect_with_regex(utterances)\n",
    "\n",
    "# Step 2: Semantic detection (slower, catches more)\n",
    "semantic_refs = []\n",
    "if CONFIG['use_bert']:\n",
    "    print(\"\\n[2/3] Semantic matching...\")\n",
    "    semantic_refs = detector.detect_with_semantic(utterances)\n",
    "\n",
    "# Step 3: Merge and deduplicate\n",
    "print(\"\\n[3/3] Merging results...\")\n",
    "all_refs = regex_refs + semantic_refs\n",
    "\n",
    "# Remove duplicates (within 2 second window)\n",
    "unique_refs = []\n",
    "seen_windows = set()\n",
    "\n",
    "for ref in sorted(all_refs, key=lambda x: (x['timestamp_ms'], -x['confidence'])):\n",
    "    # Use 2-second windows\n",
    "    window = ref['timestamp_ms'] // 2000\n",
    "    \n",
    "    if window not in seen_windows:\n",
    "        unique_refs.append(ref)\n",
    "        seen_windows.add(window)\n",
    "\n",
    "print(f\"\\nâœ… Detection complete!\")\n",
    "print(f\"   Regex matches: {len(regex_refs)}\")\n",
    "print(f\"   Semantic matches: {len(semantic_refs)}\")\n",
    "print(f\"   Unique references: {len(unique_refs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 7: Save Results\n",
    "# ====================================================================\n",
    "\n",
    "result = {\n",
    "    \"video_id\": video_id,\n",
    "    \"video_url\": phase1_data.get('video_url', ''),\n",
    "    \"references\": unique_refs,\n",
    "    \"reference_count\": len(unique_refs),\n",
    "    \"detection_summary\": {\n",
    "        \"regex_count\": len(regex_refs),\n",
    "        \"semantic_count\": len(semantic_refs),\n",
    "        \"unique_count\": len(unique_refs),\n",
    "        \"method\": \"regex+semantic\" if CONFIG['use_bert'] else \"regex_only\"\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"model\": CONFIG['bert_model'] if CONFIG['use_bert'] else None,\n",
    "        \"threshold\": CONFIG['similarity_threshold'],\n",
    "        \"phrase_count\": len(CONFIG['reference_phrases'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "output_file = f\"{CONFIG['output_dir']}/phase2_references_{video_id}.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save simple text report\n",
    "report_file = f\"{CONFIG['output_dir']}/phase2_report_{video_id}.txt\"\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"Visual Reference Detection Report\\n\")\n",
    "    f.write(f\"Video ID: {video_id}\\n\")\n",
    "    f.write(f\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    for i, ref in enumerate(unique_refs, 1):\n",
    "        time_sec = ref['timestamp_ms'] / 1000\n",
    "        f.write(f\"[{i}] {time_sec:.1f}s - {ref['detection_method']} (conf: {ref['confidence']:.2f})\\n\")\n",
    "        f.write(f\"    {ref['text']}\\n\\n\")\n",
    "\n",
    "file_size = os.path.getsize(output_file) / 1024\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… PHASE 2 COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ðŸ“Š Results:\")\n",
    "print(f\"   - Total references: {len(unique_refs)}\")\n",
    "print(f\"   - Average confidence: {np.mean([r['confidence'] for r in unique_refs]):.2f}\")\n",
    "print(f\"\\nðŸ’¾ Files saved:\")\n",
    "print(f\"   - JSON: {output_file} ({file_size:.1f} KB)\")\n",
    "print(f\"   - Report: {report_file}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 8: Display Sample Results\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\nðŸ“‹ Sample References (first 10):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, ref in enumerate(unique_refs[:10], 1):\n",
    "    time = ref['timestamp_ms'] / 1000\n",
    "    method = ref['detection_method']\n",
    "    conf = ref['confidence']\n",
    "    \n",
    "    print(f\"\\n[{i}] {time:.1f}s | {method} | confidence: {conf:.2f}\")\n",
    "    print(f\"    Type: {ref['reference_type']}\")\n",
    "    print(f\"    Text: {ref['text'][:120]}...\")\n",
    "    \n",
    "    if ref.get('context_before'):\n",
    "        print(f\"    Before: {ref['context_before'][:80]}...\")\n",
    "\n",
    "if len(unique_refs) > 10:\n",
    "    print(f\"\\n... and {len(unique_refs) - 10} more references\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 9: Statistics & Visualization\n",
    "# ====================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(unique_refs) > 0:\n",
    "    # Reference timeline\n",
    "    timestamps = [r['timestamp_ms'] / 1000 for r in unique_refs]\n",
    "    confidences = [r['confidence'] for r in unique_refs]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Timeline scatter plot\n",
    "    colors = ['blue' if r['detection_method'] == 'regex' else 'orange' for r in unique_refs]\n",
    "    ax1.scatter(timestamps, confidences, c=colors, alpha=0.6, s=100)\n",
    "    ax1.axhline(y=CONFIG['similarity_threshold'], color='red', linestyle='--', alpha=0.5, label='Threshold')\n",
    "    ax1.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax1.set_ylabel('Confidence', fontsize=12)\n",
    "    ax1.set_title('Visual References Timeline', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(['Threshold', 'Regex', 'Semantic'])\n",
    "    \n",
    "    # Type distribution\n",
    "    types = [r['reference_type'] for r in unique_refs]\n",
    "    type_counts = {t: types.count(t) for t in set(types)}\n",
    "    ax2.bar(type_counts.keys(), type_counts.values(), color='steelblue')\n",
    "    ax2.set_xlabel('Reference Type', fontsize=12)\n",
    "    ax2.set_ylabel('Count', fontsize=12)\n",
    "    ax2.set_title('Reference Type Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/phase2_analysis_{video_id}.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ðŸ“Š Visualization saved!\")\n",
    "else:\n",
    "    print(\"âš ï¸  No references found to visualize\")\n",
    "\n",
    "# Summary stats\n",
    "print(f\"\\nðŸ“ˆ Statistics:\")\n",
    "print(f\"   Average time between refs: {np.mean(np.diff(timestamps)) if len(timestamps) > 1 else 0:.1f}s\")\n",
    "print(f\"   Reference density: {len(unique_refs) / (phase1_data['duration_ms']/1000) * 60:.1f} refs/minute\")\n",
    "\n",
    "print(\"\\nâœ… All done! Download files from /kaggle/working/output/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
