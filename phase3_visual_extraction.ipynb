{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Phase 3: Visual Frame Extraction & Analysis\n",
    "## Extract frames at visual reference timestamps\n",
    "\n",
    "This notebook:\n",
    "- Loads Phase 2 references (timestamped visual references)\n",
    "- Extracts video frames at those timestamps\n",
    "- Detects duplicates using perceptual hashing\n",
    "- Optional: OCR text extraction (Tesseract/PaddleOCR)\n",
    "- Optional: AI description (supports Gemini API)\n",
    "\n",
    "**Note:** API integrations are optional - basic frame extraction works without any APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 1: Install Dependencies\n",
    "# ====================================================================\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# Core dependencies (always needed)\n",
    "!pip install -q opencv-python pillow imagehash\n",
    "\n",
    "# Optional: OCR (uncomment if you want text extraction)\n",
    "# !pip install -q pytesseract\n",
    "# !apt-get install -y tesseract-ocr\n",
    "\n",
    "# Optional: Better OCR for equations/diagrams\n",
    "# !pip install -q paddlepaddle paddleocr\n",
    "\n",
    "# Optional: Google Gemini for AI descriptions\n",
    "# !pip install -q google-generativeai\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 2: Import Libraries\n",
    "# ====================================================================\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "import shutil\n",
    "\n",
    "# Optional imports (will fail gracefully if not installed)\n",
    "try:\n",
    "    import pytesseract\n",
    "    TESSERACT_AVAILABLE = True\n",
    "except:\n",
    "    TESSERACT_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Tesseract not available (OCR disabled)\")\n",
    "\n",
    "try:\n",
    "    from paddleocr import PaddleOCR\n",
    "    PADDLE_AVAILABLE = True\n",
    "except:\n",
    "    PADDLE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  PaddleOCR not available\")\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    GEMINI_AVAILABLE = True\n",
    "except:\n",
    "    GEMINI_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Google Gemini not available\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")\n",
    "print(f\"   OCR (Tesseract): {'‚úì' if TESSERACT_AVAILABLE else '‚úó'}\")\n",
    "print(f\"   OCR (Paddle): {'‚úì' if PADDLE_AVAILABLE else '‚úó'}\")\n",
    "print(f\"   AI (Gemini): {'‚úì' if GEMINI_AVAILABLE else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 3: Configuration\n",
    "# ====================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Input files\n",
    "    \"phase1_file\": \"/kaggle/working/output/transcript_XXXXX.json\",  # ‚ö†Ô∏è UPDATE\n",
    "    \"phase2_file\": \"/kaggle/working/output/phase2_references_XXXXX.json\",  # ‚ö†Ô∏è UPDATE\n",
    "    \n",
    "    # Frame extraction\n",
    "    \"frame_offsets_seconds\": [-1, 0, 1, 2],  # Extract at -1s, 0s, +1s, +2s from reference\n",
    "    \"max_frames_per_reference\": 4,\n",
    "    \n",
    "    # Duplicate detection\n",
    "    \"enable_dedup\": True,\n",
    "    \"perceptual_hash_threshold\": 5,  # Hamming distance (0-64, lower=stricter)\n",
    "    \n",
    "    # Quality filtering\n",
    "    \"min_brightness\": 20,   # Skip very dark frames\n",
    "    \"max_brightness\": 250,  # Skip very bright/washed out frames\n",
    "    \"min_sharpness\": 50,    # Skip blurry frames\n",
    "    \n",
    "    # OCR settings\n",
    "    \"enable_ocr\": False,  # Set True to enable\n",
    "    \"ocr_engine\": \"tesseract\",  # \"tesseract\" or \"paddle\"\n",
    "    \"ocr_languages\": \"eng\",  # Language codes\n",
    "    \n",
    "    # AI description (Gemini)\n",
    "    \"enable_ai_description\": False,  # Set True to enable\n",
    "    \"gemini_api_key\": None,  # Set your API key or use env var\n",
    "    \"gemini_model\": \"gemini-1.5-flash\",\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"/kaggle/working/output\",\n",
    "    \"frames_dir\": \"/kaggle/working/frames\",\n",
    "    \"save_thumbnails\": True,  # Save smaller versions\n",
    "    \"thumbnail_size\": (640, 360)\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['frames_dir'], exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Frame offsets: {CONFIG['frame_offsets_seconds']}\")\n",
    "print(f\"   Deduplication: {'Enabled' if CONFIG['enable_dedup'] else 'Disabled'}\")\n",
    "print(f\"   OCR: {'Enabled (' + CONFIG['ocr_engine'] + ')' if CONFIG['enable_ocr'] else 'Disabled'}\")\n",
    "print(f\"   AI Description: {'Enabled' if CONFIG['enable_ai_description'] else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 4: Load Phase 1 & 2 Data\n",
    "# ====================================================================\n",
    "\n",
    "print(\"üìÇ Loading previous phase outputs...\")\n",
    "\n",
    "# Load Phase 1 (transcript)\n",
    "with open(CONFIG['phase1_file'], 'r', encoding='utf-8') as f:\n",
    "    phase1_data = json.load(f)\n",
    "\n",
    "# Load Phase 2 (references)\n",
    "with open(CONFIG['phase2_file'], 'r', encoding='utf-8') as f:\n",
    "    phase2_data = json.load(f)\n",
    "\n",
    "video_path = phase1_data['video_path']\n",
    "video_id = phase1_data['video_id']\n",
    "references = phase2_data['references']\n",
    "\n",
    "print(f\"‚úÖ Data loaded:\")\n",
    "print(f\"   Video: {Path(video_path).name}\")\n",
    "print(f\"   Video ID: {video_id}\")\n",
    "print(f\"   References: {len(references)}\")\n",
    "\n",
    "if len(references) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No references found! Phase 2 didn't detect any visual references.\")\n",
    "    print(\"   Try lowering the similarity threshold in Phase 2.\")\n",
    "else:\n",
    "    print(f\"\\nüìç First reference: {references[0]['timestamp_ms']/1000:.1f}s\")\n",
    "    print(f\"   Text: {references[0]['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frame_extractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 5: Frame Extraction Functions\n",
    "# ====================================================================\n",
    "\n",
    "class FrameExtractor:\n",
    "    \"\"\"Extract frames from video at specified timestamps.\"\"\"\n",
    "    \n",
    "    def __init__(self, video_path: str):\n",
    "        self.video_path = video_path\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if not self.cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    def extract_frame_at_timestamp(self, timestamp_ms: int, output_path: str) -> bool:\n",
    "        \"\"\"Extract single frame at timestamp.\"\"\"\n",
    "        # Convert timestamp to frame number\n",
    "        frame_num = int((timestamp_ms / 1000.0) * self.fps)\n",
    "        \n",
    "        # Validate frame number\n",
    "        if frame_num < 0 or frame_num >= self.total_frames:\n",
    "            return False\n",
    "        \n",
    "        # Seek to frame\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = self.cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            return False\n",
    "        \n",
    "        # Save frame\n",
    "        cv2.imwrite(output_path, frame)\n",
    "        return True\n",
    "    \n",
    "    def extract_frames_with_offsets(self, base_timestamp_ms: int, \n",
    "                                    offsets_seconds: List[float],\n",
    "                                    output_dir: str,\n",
    "                                    base_name: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract multiple frames around a timestamp.\"\"\"\n",
    "        frames = []\n",
    "        \n",
    "        for i, offset in enumerate(offsets_seconds):\n",
    "            timestamp_ms = base_timestamp_ms + int(offset * 1000)\n",
    "            output_path = os.path.join(output_dir, f\"{base_name}_offset_{offset:+.1f}s.jpg\")\n",
    "            \n",
    "            success = self.extract_frame_at_timestamp(timestamp_ms, output_path)\n",
    "            \n",
    "            if success:\n",
    "                frames.append({\n",
    "                    \"path\": output_path,\n",
    "                    \"timestamp_ms\": timestamp_ms,\n",
    "                    \"offset_seconds\": offset\n",
    "                })\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Release video capture.\"\"\"\n",
    "        self.cap.release()\n",
    "\n",
    "print(\"‚úÖ Frame extractor defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality_assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 6: Quality Assessment & Deduplication\n",
    "# ====================================================================\n",
    "\n",
    "def assess_frame_quality(image_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Assess frame quality (brightness, sharpness).\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Brightness (mean pixel value)\n",
    "    brightness = np.mean(gray)\n",
    "    \n",
    "    # Sharpness (Laplacian variance)\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    sharpness = laplacian.var()\n",
    "    \n",
    "    # Overall quality score (0-1)\n",
    "    brightness_score = 1 - abs(brightness - 127.5) / 127.5  # Prefer mid-range\n",
    "    sharpness_score = min(sharpness / 500, 1.0)  # Normalize\n",
    "    quality_score = (brightness_score * 0.3 + sharpness_score * 0.7)\n",
    "    \n",
    "    return {\n",
    "        \"brightness\": float(brightness),\n",
    "        \"sharpness\": float(sharpness),\n",
    "        \"quality_score\": float(quality_score),\n",
    "        \"is_good_quality\": (\n",
    "            CONFIG['min_brightness'] < brightness < CONFIG['max_brightness'] and\n",
    "            sharpness > CONFIG['min_sharpness']\n",
    "        )\n",
    "    }\n",
    "\n",
    "def calculate_perceptual_hash(image_path: str) -> str:\n",
    "    \"\"\"Calculate perceptual hash for duplicate detection.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    phash = imagehash.phash(img)\n",
    "    return str(phash)\n",
    "\n",
    "def is_duplicate(phash: str, seen_hashes: List[str], threshold: int = 5) -> bool:\n",
    "    \"\"\"Check if frame is duplicate based on perceptual hash.\"\"\"\n",
    "    for seen_hash in seen_hashes:\n",
    "        hash1 = imagehash.hex_to_hash(phash)\n",
    "        hash2 = imagehash.hex_to_hash(seen_hash)\n",
    "        distance = hash1 - hash2\n",
    "        \n",
    "        if distance <= threshold:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def create_thumbnail(image_path: str, output_path: str, size: Tuple[int, int]):\n",
    "    \"\"\"Create thumbnail of image.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    img.thumbnail(size, Image.Resampling.LANCZOS)\n",
    "    img.save(output_path, \"JPEG\", quality=85)\n",
    "\n",
    "print(\"‚úÖ Quality assessment functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ocr_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 7: OCR Functions (Optional)\n",
    "# ====================================================================\n",
    "\n",
    "def extract_text_tesseract(image_path: str, lang: str = \"eng\") -> Dict[str, Any]:\n",
    "    \"\"\"Extract text using Tesseract OCR.\"\"\"\n",
    "    if not TESSERACT_AVAILABLE:\n",
    "        return {\"text\": \"\", \"error\": \"Tesseract not available\"}\n",
    "    \n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(img, lang=lang)\n",
    "        confidence = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "        \n",
    "        # Calculate average confidence\n",
    "        confs = [c for c in confidence['conf'] if c != -1]\n",
    "        avg_conf = np.mean(confs) if confs else 0\n",
    "        \n",
    "        return {\n",
    "            \"text\": text.strip(),\n",
    "            \"confidence\": float(avg_conf),\n",
    "            \"engine\": \"tesseract\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"text\": \"\", \"error\": str(e)}\n",
    "\n",
    "def extract_text_paddle(image_path: str, lang: str = \"en\") -> Dict[str, Any]:\n",
    "    \"\"\"Extract text using PaddleOCR (better for diagrams/equations).\"\"\"\n",
    "    if not PADDLE_AVAILABLE:\n",
    "        return {\"text\": \"\", \"error\": \"PaddleOCR not available\"}\n",
    "    \n",
    "    try:\n",
    "        ocr = PaddleOCR(use_angle_cls=True, lang=lang, show_log=False)\n",
    "        result = ocr.ocr(image_path, cls=True)\n",
    "        \n",
    "        # Extract text and confidence\n",
    "        texts = []\n",
    "        confidences = []\n",
    "        \n",
    "        if result and result[0]:\n",
    "            for line in result[0]:\n",
    "                texts.append(line[1][0])\n",
    "                confidences.append(line[1][1])\n",
    "        \n",
    "        return {\n",
    "            \"text\": \"\\n\".join(texts),\n",
    "            \"confidence\": float(np.mean(confidences)) if confidences else 0,\n",
    "            \"engine\": \"paddle\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"text\": \"\", \"error\": str(e)}\n",
    "\n",
    "def extract_text_from_frame(image_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract text using configured OCR engine.\"\"\"\n",
    "    if not CONFIG['enable_ocr']:\n",
    "        return None\n",
    "    \n",
    "    if CONFIG['ocr_engine'] == 'paddle':\n",
    "        return extract_text_paddle(image_path, CONFIG['ocr_languages'])\n",
    "    else:\n",
    "        return extract_text_tesseract(image_path, CONFIG['ocr_languages'])\n",
    "\n",
    "print(\"‚úÖ OCR functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ai_description",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 8: AI Description (Gemini - Optional)\n",
    "# ====================================================================\n",
    "\n",
    "def generate_ai_description(image_path: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Generate AI description using Google Gemini.\"\"\"\n",
    "    if not CONFIG['enable_ai_description']:\n",
    "        return None\n",
    "    \n",
    "    if not GEMINI_AVAILABLE:\n",
    "        return {\"description\": \"\", \"error\": \"Gemini not available\"}\n",
    "    \n",
    "    try:\n",
    "        # Configure API\n",
    "        api_key = CONFIG['gemini_api_key'] or os.getenv('GEMINI_API_KEY')\n",
    "        if not api_key:\n",
    "            return {\"description\": \"\", \"error\": \"No API key provided\"}\n",
    "        \n",
    "        genai.configure(api_key=api_key)\n",
    "        model = genai.GenerativeModel(CONFIG['gemini_model'])\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Generate description\n",
    "        prompt = \"\"\"\n",
    "        Analyze this educational video frame and provide:\n",
    "        1. A concise description of what's shown (1-2 sentences)\n",
    "        2. Any text, equations, or diagrams visible\n",
    "        3. The main educational concept being illustrated\n",
    "        \n",
    "        Format as JSON: {\"description\": \"...\", \"type\": \"diagram/text/equation/mixed\", \"concept\": \"...\"}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = model.generate_content([prompt, img])\n",
    "        \n",
    "        # Try to parse JSON response\n",
    "        try:\n",
    "            result = json.loads(response.text)\n",
    "        except:\n",
    "            result = {\n",
    "                \"description\": response.text,\n",
    "                \"type\": \"unknown\",\n",
    "                \"concept\": \"\"\n",
    "            }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"description\": \"\", \"error\": str(e)}\n",
    "\n",
    "print(\"‚úÖ AI description function defined\")\n",
    "if CONFIG['enable_ai_description'] and not CONFIG['gemini_api_key']:\n",
    "    print(\"‚ö†Ô∏è  To use Gemini, set CONFIG['gemini_api_key'] or GEMINI_API_KEY env var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 9: Main Processing Loop\n",
    "# ====================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING PHASE 3: VISUAL EXTRACTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = FrameExtractor(video_path)\n",
    "\n",
    "all_frames = []\n",
    "seen_hashes = []\n",
    "total_extracted = 0\n",
    "duplicates_skipped = 0\n",
    "low_quality_skipped = 0\n",
    "\n",
    "print(f\"\\nProcessing {len(references)} references...\\n\")\n",
    "\n",
    "for ref_idx, reference in enumerate(references, 1):\n",
    "    ref_id = reference['reference_id']\n",
    "    timestamp_ms = reference['timestamp_ms']\n",
    "    \n",
    "    print(f\"[{ref_idx}/{len(references)}] {ref_id} @ {timestamp_ms/1000:.1f}s\")\n",
    "    print(f\"  Text: {reference['text'][:80]}...\")\n",
    "    \n",
    "    # Create reference directory\n",
    "    ref_dir = os.path.join(CONFIG['frames_dir'], ref_id)\n",
    "    os.makedirs(ref_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract frames with offsets\n",
    "    frames = extractor.extract_frames_with_offsets(\n",
    "        timestamp_ms,\n",
    "        CONFIG['frame_offsets_seconds'],\n",
    "        ref_dir,\n",
    "        ref_id\n",
    "    )\n",
    "    \n",
    "    print(f\"  Extracted {len(frames)} frames\")\n",
    "    \n",
    "    # Process each frame\n",
    "    for frame in frames:\n",
    "        total_extracted += 1\n",
    "        frame_path = frame['path']\n",
    "        \n",
    "        # Quality assessment\n",
    "        quality = assess_frame_quality(frame_path)\n",
    "        \n",
    "        if not quality['is_good_quality']:\n",
    "            print(f\"    ‚ö†Ô∏è  Low quality (brightness={quality['brightness']:.0f}, sharpness={quality['sharpness']:.0f})\")\n",
    "            low_quality_skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Duplicate detection\n",
    "        phash = calculate_perceptual_hash(frame_path)\n",
    "        is_dup = is_duplicate(phash, seen_hashes, CONFIG['perceptual_hash_threshold']) if CONFIG['enable_dedup'] else False\n",
    "        \n",
    "        if is_dup:\n",
    "            print(f\"    üîÑ Duplicate detected (skipping)\")\n",
    "            duplicates_skipped += 1\n",
    "            continue\n",
    "        \n",
    "        seen_hashes.append(phash)\n",
    "        \n",
    "        # Create thumbnail\n",
    "        if CONFIG['save_thumbnails']:\n",
    "            thumb_path = frame_path.replace('.jpg', '_thumb.jpg')\n",
    "            create_thumbnail(frame_path, thumb_path, CONFIG['thumbnail_size'])\n",
    "        else:\n",
    "            thumb_path = None\n",
    "        \n",
    "        # OCR extraction\n",
    "        ocr_data = extract_text_from_frame(frame_path)\n",
    "        if ocr_data and ocr_data.get('text'):\n",
    "            print(f\"    üìù OCR: {ocr_data['text'][:60]}...\")\n",
    "        \n",
    "        # AI description\n",
    "        ai_desc = generate_ai_description(frame_path)\n",
    "        if ai_desc and ai_desc.get('description'):\n",
    "            print(f\"    ü§ñ AI: {ai_desc['description'][:60]}...\")\n",
    "        \n",
    "        # Store frame info\n",
    "        frame_info = {\n",
    "            \"frame_id\": f\"{ref_id}_F{len(all_frames)}\",\n",
    "            \"reference_id\": ref_id,\n",
    "            \"reference_text\": reference['text'],\n",
    "            \"timestamp_ms\": frame['timestamp_ms'],\n",
    "            \"offset_seconds\": frame['offset_seconds'],\n",
    "            \"frame_path\": frame_path,\n",
    "            \"thumbnail_path\": thumb_path,\n",
    "            \"perceptual_hash\": phash,\n",
    "            \"quality\": quality,\n",
    "            \"ocr_data\": ocr_data,\n",
    "            \"ai_description\": ai_desc\n",
    "        }\n",
    "        \n",
    "        all_frames.append(frame_info)\n",
    "        print(f\"    ‚úÖ Processed (quality={quality['quality_score']:.2f})\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Cleanup\n",
    "extractor.close()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ EXTRACTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Statistics:\")\n",
    "print(f\"   Total extracted: {total_extracted}\")\n",
    "print(f\"   Duplicates skipped: {duplicates_skipped}\")\n",
    "print(f\"   Low quality skipped: {low_quality_skipped}\")\n",
    "print(f\"   Unique frames kept: {len(all_frames)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 10: Save Results\n",
    "# ====================================================================\n",
    "\n",
    "result = {\n",
    "    \"video_id\": video_id,\n",
    "    \"video_path\": video_path,\n",
    "    \"frames\": all_frames,\n",
    "    \"frame_count\": len(all_frames),\n",
    "    \"statistics\": {\n",
    "        \"total_extracted\": total_extracted,\n",
    "        \"duplicates_skipped\": duplicates_skipped,\n",
    "        \"low_quality_skipped\": low_quality_skipped,\n",
    "        \"unique_frames\": len(all_frames)\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"frame_offsets\": CONFIG['frame_offsets_seconds'],\n",
    "        \"dedup_enabled\": CONFIG['enable_dedup'],\n",
    "        \"ocr_enabled\": CONFIG['enable_ocr'],\n",
    "        \"ai_enabled\": CONFIG['enable_ai_description']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "output_file = f\"{CONFIG['output_dir']}/phase3_frames_{video_id}.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save text report\n",
    "report_file = f\"{CONFIG['output_dir']}/phase3_report_{video_id}.txt\"\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Visual Frame Extraction Report\\n\")\n",
    "    f.write(f\"Video ID: {video_id}\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    for i, frame in enumerate(all_frames, 1):\n",
    "        f.write(f\"[{i}] Frame ID: {frame['frame_id']}\\n\")\n",
    "        f.write(f\"    Time: {frame['timestamp_ms']/1000:.1f}s (offset: {frame['offset_seconds']:+.1f}s)\\n\")\n",
    "        f.write(f\"    Reference: {frame['reference_text'][:80]}...\\n\")\n",
    "        f.write(f\"    Quality: {frame['quality']['quality_score']:.2f}\\n\")\n",
    "        f.write(f\"    Path: {frame['frame_path']}\\n\")\n",
    "        \n",
    "        if frame.get('ocr_data') and frame['ocr_data'].get('text'):\n",
    "            f.write(f\"    OCR: {frame['ocr_data']['text'][:100]}...\\n\")\n",
    "        \n",
    "        if frame.get('ai_description') and frame['ai_description'].get('description'):\n",
    "            f.write(f\"    AI: {frame['ai_description']['description'][:100]}...\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "\n",
    "file_size = os.path.getsize(output_file) / 1024\n",
    "\n",
    "print(\"\\nüíæ Files saved:\")\n",
    "print(f\"   JSON: {output_file} ({file_size:.1f} KB)\")\n",
    "print(f\"   Report: {report_file}\")\n",
    "print(f\"   Frames: {CONFIG['frames_dir']}/\")\n",
    "print(\"\\n‚úÖ Phase 3 complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 11: Display Sample Frames\n",
    "# ====================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "if len(all_frames) > 0:\n",
    "    # Show first 6 frames\n",
    "    num_samples = min(6, len(all_frames))\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    gs = gridspec.GridSpec(2, 3, hspace=0.3, wspace=0.2)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        frame = all_frames[i]\n",
    "        \n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        \n",
    "        # Load and display image\n",
    "        img = cv2.imread(frame['frame_path'])\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img_rgb)\n",
    "        \n",
    "        # Title with info\n",
    "        time = frame['timestamp_ms'] / 1000\n",
    "        quality = frame['quality']['quality_score']\n",
    "        title = f\"{frame['frame_id']}\\n{time:.1f}s | Q={quality:.2f}\"\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Sample Extracted Frames ({num_samples}/{len(all_frames)})\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/phase3_samples_{video_id}.png\", \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüì∏ Sample visualization saved!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No frames to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timeline_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 12: Timeline Visualization\n",
    "# ====================================================================\n",
    "\n",
    "if len(all_frames) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(16, 4))\n",
    "    \n",
    "    # Plot frames on timeline\n",
    "    timestamps = [f['timestamp_ms'] / 1000 for f in all_frames]\n",
    "    qualities = [f['quality']['quality_score'] for f in all_frames]\n",
    "    \n",
    "    scatter = ax.scatter(timestamps, qualities, \n",
    "                        s=100, alpha=0.6, c=qualities, \n",
    "                        cmap='viridis', edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Add reference markers\n",
    "    ref_times = [r['timestamp_ms'] / 1000 for r in references]\n",
    "    for t in ref_times:\n",
    "        ax.axvline(x=t, color='red', alpha=0.3, linestyle='--', linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax.set_ylabel('Frame Quality Score', fontsize=12)\n",
    "    ax.set_title('Extracted Frames Timeline\\n(Red lines = reference timestamps)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Quality Score', ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/phase3_timeline_{video_id}.png\", \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Timeline visualization saved!\")\n",
    "\n",
    "print(\"\\n‚úÖ All visualizations complete!\")\n",
    "print(f\"\\nüìÅ Download all files from: {CONFIG['output_dir']}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
