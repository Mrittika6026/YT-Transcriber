{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# YouTube Video Transcription Pipeline\n## Optimized for Kaggle - No OOM Errors\n\nThis notebook:\n- Downloads YouTube videos\n- Extracts audio\n- Transcribes using OpenAI Whisper (memory-efficient)\n- Creates timestamped utterances\n- Exports JSON output","metadata":{}},{"cell_type":"code","source":"# ====================================================================\n# CELL 1: Install Dependencies\n# ====================================================================\nprint(\"ðŸ“¦ Installing dependencies...\")\n!pip install -q yt-dlp openai-whisper\nprint(\"âœ… Dependencies installed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:46:59.125426Z","iopub.execute_input":"2026-02-09T08:46:59.125815Z","iopub.status.idle":"2026-02-09T08:47:12.265167Z","shell.execute_reply.started":"2026-02-09T08:46:59.125787Z","shell.execute_reply":"2026-02-09T08:47:12.264332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 2: Import Libraries\n# ====================================================================\nimport os\nimport json\nimport subprocess\nimport gc\nfrom pathlib import Path\nimport whisper\nimport torch\n\nprint(f\"ðŸ–¥ï¸  Device: {'CUDA (GPU)' if torch.cuda.is_available() else 'CPU'}\")\nif torch.cuda.is_available():\n    print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    # Clear any existing cache\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:19.012435Z","iopub.execute_input":"2026-02-09T08:47:19.013214Z","iopub.status.idle":"2026-02-09T08:47:24.298613Z","shell.execute_reply.started":"2026-02-09T08:47:19.013179Z","shell.execute_reply":"2026-02-09T08:47:24.297792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 3: Configuration\n# ====================================================================\n# âš ï¸ CHANGE THIS to your YouTube video URL\nVIDEO_URL = \"https://www.youtube.com/watch?v=XNQTWZ87K4I\"  # Replace with your video\n\n# Model size options (smaller = less memory):\n# \"tiny\"   - ~1GB VRAM, fastest, least accurate\n# \"base\"   - ~1GB VRAM, fast, decent accuracy  âœ… RECOMMENDED for Kaggle\n# \"small\"  - ~2GB VRAM, good accuracy\n# \"medium\" - ~5GB VRAM, better accuracy\n# \"large\"  - ~10GB VRAM, best accuracy (may OOM on Kaggle)\n\nCONFIG = {\n    \"model_size\": \"base\",  # Change to \"small\" or \"medium\" if you have enough memory\n    \"language\": \"en\",      # \"en\" for English, \"es\" for Spanish, None for auto-detect\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"output_dir\": \"/kaggle/working/output\"\n}\n\n# Create directories\nos.makedirs(\"/kaggle/working/videos\", exist_ok=True)\nos.makedirs(\"/kaggle/working/audio\", exist_ok=True)\nos.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n\nprint(\"âœ… Configuration loaded\")\nprint(f\"   Model: {CONFIG['model_size']}\")\nprint(f\"   Language: {CONFIG['language'] or 'auto-detect'}\")\nprint(f\"   Device: {CONFIG['device']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:30.612694Z","iopub.execute_input":"2026-02-09T08:47:30.613571Z","iopub.status.idle":"2026-02-09T08:47:30.620294Z","shell.execute_reply.started":"2026-02-09T08:47:30.613538Z","shell.execute_reply":"2026-02-09T08:47:30.619538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 4: Download Video\n# ====================================================================\ndef download_youtube_video(url: str):\n    \"\"\"Download YouTube video using yt-dlp.\"\"\"\n    print(f\"ðŸ“¥ Downloading video from: {url}\")\n    \n    import yt_dlp\n    \n    output_dir = \"/kaggle/working/videos\"\n    \n    ydl_opts = {\n        'format': 'best[ext=mp4]/best',\n        'outtmpl': os.path.join(output_dir, '%(id)s.%(ext)s'),\n        'quiet': True,\n        'no_warnings': True,\n    }\n    \n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        info = ydl.extract_info(url, download=True)\n        video_id = info['id']\n        video_title = info.get('title', 'Unknown')\n        duration = info.get('duration', 0)\n        video_path = os.path.join(output_dir, f\"{video_id}.mp4\")\n    \n    print(f\"âœ… Video downloaded: {video_title}\")\n    print(f\"   Duration: {duration//60}m {duration%60}s\")\n    return video_path, video_id, video_title, duration\n\n# Download the video\nvideo_path, video_id, video_title, video_duration = download_youtube_video(VIDEO_URL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:33.943069Z","iopub.execute_input":"2026-02-09T08:47:33.943506Z","iopub.status.idle":"2026-02-09T08:47:36.535156Z","shell.execute_reply.started":"2026-02-09T08:47:33.943472Z","shell.execute_reply":"2026-02-09T08:47:36.534441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 5: Extract Audio\n# ====================================================================\ndef extract_audio(video_path: str) -> str:\n    \"\"\"Extract audio from video using ffmpeg.\"\"\"\n    print(f\"ðŸŽµ Extracting audio from: {Path(video_path).name}\")\n    \n    video_name = Path(video_path).stem\n    audio_path = f\"/kaggle/working/audio/{video_name}.mp3\"\n    \n    # Extract as MP3 (Whisper handles this well)\n    cmd = [\n        'ffmpeg', '-i', video_path,\n        '-vn', '-acodec', 'libmp3lame',\n        '-ar', '16000', '-ac', '1',\n        '-b:a', '64k',\n        '-y', audio_path\n    ]\n    \n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        raise Exception(f\"FFmpeg error: {result.stderr}\")\n    \n    file_size = os.path.getsize(audio_path) / (1024*1024)\n    print(f\"âœ… Audio extracted: {file_size:.2f} MB\")\n    return audio_path\n\n# Extract audio\naudio_path = extract_audio(video_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:39.663677Z","iopub.execute_input":"2026-02-09T08:47:39.664137Z","iopub.status.idle":"2026-02-09T08:47:41.713204Z","shell.execute_reply.started":"2026-02-09T08:47:39.664105Z","shell.execute_reply":"2026-02-09T08:47:41.712428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 6: Transcribe Audio with Whisper\n# ====================================================================\ndef transcribe_with_whisper(audio_path: str, model_size: str, language: str = None):\n    \"\"\"Transcribe audio using Whisper (memory-efficient).\"\"\"\n    print(f\"ðŸŽ¤ Loading Whisper model: {model_size}\")\n    \n    # Load model with FP16 if on GPU (saves memory)\n    model = whisper.load_model(\n        model_size,\n        device=CONFIG['device']\n    )\n    \n    print(f\"ðŸŽ¯ Transcribing audio...\")\n    print(\"   This may take a few minutes depending on video length\")\n    \n    # Transcribe with options\n    result = model.transcribe(\n        audio_path,\n        language=language,\n        fp16=torch.cuda.is_available(),  # Use FP16 on GPU (faster, less memory)\n        verbose=False,\n        word_timestamps=True  # Get word-level timestamps\n    )\n    \n    # Clean up model to free memory\n    del model\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(f\"âœ… Transcription complete!\")\n    print(f\"   Detected language: {result['language']}\")\n    print(f\"   Segments: {len(result['segments'])}\")\n    \n    return result\n\n# Transcribe\ntranscription = transcribe_with_whisper(\n    audio_path,\n    CONFIG['model_size'],\n    CONFIG['language']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:44.267913Z","iopub.execute_input":"2026-02-09T08:47:44.268528Z","iopub.status.idle":"2026-02-09T08:48:02.693602Z","shell.execute_reply.started":"2026-02-09T08:47:44.268483Z","shell.execute_reply":"2026-02-09T08:48:02.692737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 7: Process Results\n# ====================================================================\ndef create_utterances(transcription_result):\n    \"\"\"Convert Whisper segments to utterance format.\"\"\"\n    utterances = []\n    \n    for segment in transcription_result['segments']:\n        utterances.append({\n            \"text\": segment['text'].strip(),\n            \"start_ms\": int(segment['start'] * 1000),\n            \"end_ms\": int(segment['end'] * 1000),\n            \"confidence\": segment.get('confidence', 0.0),\n            \"speaker\": \"default\",\n            \"words\": segment.get('words', [])  # Word-level timestamps if available\n        })\n    \n    return utterances\n\n# Create utterances\nutterances = create_utterances(transcription)\nfull_transcript = transcription['text']\n\nprint(f\"âœ… Created {len(utterances)} utterances\")\nprint(f\"\\nðŸ“ Transcript preview (first 500 chars):\")\nprint(\"=\"*70)\nprint(full_transcript[:500] + \"...\" if len(full_transcript) > 500 else full_transcript)\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:48:06.930775Z","iopub.execute_input":"2026-02-09T08:48:06.931114Z","iopub.status.idle":"2026-02-09T08:48:06.938072Z","shell.execute_reply.started":"2026-02-09T08:48:06.931086Z","shell.execute_reply":"2026-02-09T08:48:06.937282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 8: Save Results\n# ====================================================================\n# Prepare final output\nresult = {\n    \"video_id\": video_id,\n    \"video_title\": video_title,\n    \"video_url\": VIDEO_URL,\n    \"video_path\": video_path,\n    \"audio_path\": audio_path,\n    \"duration_ms\": video_duration * 1000,\n    \"full_transcript\": full_transcript,\n    \"utterances\": utterances,\n    \"utterance_count\": len(utterances),\n    \"model_used\": f\"whisper-{CONFIG['model_size']}\",\n    \"language\": transcription['language']\n}\n\n# Save to JSON\noutput_file = f\"{CONFIG['output_dir']}/transcript_{video_id}.json\"\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(result, f, ensure_ascii=False, indent=2)\n\n# Also save just the text\ntext_file = f\"{CONFIG['output_dir']}/transcript_{video_id}.txt\"\nwith open(text_file, 'w', encoding='utf-8') as f:\n    f.write(full_transcript)\n\nfile_size = os.path.getsize(output_file) / (1024*1024)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… TRANSCRIPTION COMPLETE!\")\nprint(\"=\"*70)\nprint(f\"ðŸ“Š Results:\")\nprint(f\"   - Video: {video_title}\")\nprint(f\"   - Duration: {video_duration//60}m {video_duration%60}s\")\nprint(f\"   - Language: {transcription['language']}\")\nprint(f\"   - Utterances: {len(utterances)}\")\nprint(f\"   - Transcript length: {len(full_transcript)} characters\")\nprint(f\"\\nðŸ’¾ Files saved:\")\nprint(f\"   - JSON: {output_file} ({file_size:.2f} MB)\")\nprint(f\"   - Text: {text_file}\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:48:11.146850Z","iopub.execute_input":"2026-02-09T08:48:11.147192Z","iopub.status.idle":"2026-02-09T08:48:11.164491Z","shell.execute_reply.started":"2026-02-09T08:48:11.147164Z","shell.execute_reply":"2026-02-09T08:48:11.163649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 9: Display Sample Results\n# ====================================================================\nprint(\"\\nðŸ“„ First 5 utterances:\")\nprint(\"=\"*70)\nfor i, utt in enumerate(utterances[:5], 1):\n    start_time = utt['start_ms'] / 1000\n    end_time = utt['end_ms'] / 1000\n    print(f\"\\n[{i}] {start_time:.1f}s - {end_time:.1f}s\")\n    print(f\"    {utt['text']}\")\n    if utt.get('confidence'):\n        print(f\"    Confidence: {utt['confidence']:.2f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… All done! Download your files from /kaggle/working/output/\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:48:14.562011Z","iopub.execute_input":"2026-02-09T08:48:14.562786Z","iopub.status.idle":"2026-02-09T08:48:14.568420Z","shell.execute_reply.started":"2026-02-09T08:48:14.562755Z","shell.execute_reply":"2026-02-09T08:48:14.567639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Phase 2**","metadata":{}},{"cell_type":"code","source":"# ====================================================================\n# CELL 1: Install Dependencies\n# ====================================================================\nprint(\"ðŸ“¦ Installing dependencies...\")\n!pip install -q sentence-transformers torch numpy\nprint(\"âœ… Dependencies installed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:48:18.494402Z","iopub.execute_input":"2026-02-09T08:48:18.495305Z","iopub.status.idle":"2026-02-09T08:48:22.283459Z","shell.execute_reply.started":"2026-02-09T08:48:18.495226Z","shell.execute_reply":"2026-02-09T08:48:22.282542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 2: Import Libraries\n# ====================================================================\nimport os\nimport json\nimport re\nfrom typing import List, Dict, Any, Tuple\nimport torch\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport gc\n\nprint(f\"ðŸ–¥ï¸  Device: {'CUDA (GPU)' if torch.cuda.is_available() else 'CPU'}\")\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:48:23.295858Z","iopub.execute_input":"2026-02-09T08:48:23.296545Z","iopub.status.idle":"2026-02-09T08:48:51.810484Z","shell.execute_reply.started":"2026-02-09T08:48:23.296503Z","shell.execute_reply":"2026-02-09T08:48:51.809395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 3: Configuration\n# ====================================================================\n\nCONFIG = {\n    \"phase1_file\": \"/kaggle/working/output/transcript_XNQTWZ87K4I.json\", \n    \"use_bert\": True,\n    \"bert_model\": \"all-MiniLM-L6-v2\",\n    \"similarity_threshold\": 0.38,  # Slightly lower to catch semantic variations\n    \n    \"reference_phrases\": [\n        # Navigation/Showing Actions\n        \"take a look on\",\n        \"as you see right here\",\n        \"show you how it can be done\",\n        \"if you take a look\",\n        \"if I go to\",\n        \"press refresh\",\n        \"inside the administration panel\",\n        \n        # Specific Visual Objects in Video\n        \"cookie banner\",\n        \"detailed cookie banner\",\n        \"little widget\",\n        \"trash bag\",\n        \"consent tracking\",\n        \"record of the consents\",\n        \n        # Positional/Structural references\n        \"down here\",\n        \"inside our back-end\",\n        \"second part is\",\n        \"third point is\"\n    ],\n    \"output_dir\": \"/kaggle/working/output\"\n}\n\nos.makedirs(CONFIG['output_dir'], exist_ok=True)\nprint(\"âœ… Configuration loaded\")\nprint(f\"   BERT: {'Enabled' if CONFIG['use_bert'] else 'Disabled'}\")\nprint(f\"   Similarity threshold: {CONFIG['similarity_threshold']}\")\nprint(f\"   Reference phrases: {len(CONFIG['reference_phrases'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:48:54.998300Z","iopub.execute_input":"2026-02-09T08:48:54.999102Z","iopub.status.idle":"2026-02-09T08:48:55.006901Z","shell.execute_reply.started":"2026-02-09T08:48:54.999064Z","shell.execute_reply":"2026-02-09T08:48:55.005939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 4: Load Phase 1 Output\n# ====================================================================\n\nprint(f\"ðŸ“‚ Loading Phase 1 transcript from: {CONFIG['phase1_file']}\")\n\nwith open(CONFIG['phase1_file'], 'r', encoding='utf-8') as f:\n    phase1_data = json.load(f)\n\nutterances = phase1_data['utterances']\nvideo_id = phase1_data['video_id']\n\nprint(f\"âœ… Loaded transcript:\")\nprint(f\"   Video ID: {video_id}\")\nprint(f\"   Utterances: {len(utterances)}\")\nprint(f\"   Duration: {phase1_data['duration_ms']/1000:.1f}s\")\nprint(f\"\\nðŸ“ First utterance: {utterances[0]['text'][:100]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:48:58.039769Z","iopub.execute_input":"2026-02-09T08:48:58.040513Z","iopub.status.idle":"2026-02-09T08:48:58.047940Z","shell.execute_reply.started":"2026-02-09T08:48:58.040470Z","shell.execute_reply":"2026-02-09T08:48:58.047191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 5: Reference Detector Class\n# ====================================================================\n\nclass EnglishReferenceDetector:\n    \"\"\"Detect visual references in English transcripts.\"\"\"\n    \n    def __init__(self, phrases: List[str], model_name: str = None,\n                 use_bert: bool = True, threshold: float = 0.38):\n        self.phrases = phrases\n        self.use_bert = use_bert\n        self.threshold = threshold\n        self.model = None\n        \n        self.patterns = [\n            re.compile(r'\\b' + re.escape(phrase) + r'\\b', re.IGNORECASE)\n            for phrase in phrases\n        ]\n        \n        if use_bert and model_name:\n            print(f\"ðŸ¤– Loading semantic model: {model_name}\")\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            self.model = SentenceTransformer(model_name, device=device)\n            print(f\"âœ… Model loaded on {device}\")\n    \n    def detect_with_regex(self, utterances: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Detect references using regex pattern matching.\"\"\"\n        print(\"ðŸ” Detecting with regex patterns...\")\n        references = []\n        reference_id = 1\n        \n        for utt_idx, utterance in enumerate(utterances):\n            text = utterance['text']\n            \n            for phrase, pattern in zip(self.phrases, self.patterns):\n                match = pattern.search(text)\n                \n                if match:\n                    references.append({\n                        \"reference_id\": f\"REF_{reference_id:03d}\",\n                        \"text\": text,\n                        \"timestamp_ms\": utterance['start_ms'],\n                        \"end_ms\": utterance['end_ms'],\n                        \"matched_phrase\": phrase,\n                        \"reference_type\": self._classify_type(phrase),\n                        \"detection_method\": \"regex\",\n                        \"confidence\": 0.95,\n                        \"context_before\": self._get_context(utterances, utt_idx, -1),\n                        \"context_after\": self._get_context(utterances, utt_idx, 1)\n                    })\n                    reference_id += 1\n                    break  # Only count once per utterance\n        \n        print(f\"   Found {len(references)} regex matches\")\n        return references\n    \n    def detect_with_semantic(self, utterances: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Detect references using semantic similarity.\"\"\"\n        if not self.model:\n            print(\"âš ï¸  Semantic model not loaded, skipping\")\n            return []\n        \n        print(f\"ðŸ§  Detecting with semantic matching (threshold={self.threshold})...\")\n        references = []\n        reference_id = 1000  # Different ID range for semantic\n        \n        # Encode reference phrases\n        print(\"   Encoding reference phrases...\")\n        reference_embeddings = self.model.encode(\n            self.phrases,\n            convert_to_tensor=True,\n            show_progress_bar=False\n        )\n        \n        # Process in batches to save memory\n        batch_size = 32\n        print(f\"   Processing {len(utterances)} utterances in batches...\")\n        \n        for batch_start in range(0, len(utterances), batch_size):\n            batch_end = min(batch_start + batch_size, len(utterances))\n            batch = utterances[batch_start:batch_end]\n            \n            # Encode batch\n            texts = [u['text'] for u in batch]\n            text_embeddings = self.model.encode(\n                texts,\n                convert_to_tensor=True,\n                show_progress_bar=False\n            )\n            \n            # Calculate similarities\n            similarities = torch.mm(text_embeddings, reference_embeddings.T)\n            max_sims, _ = similarities.max(dim=1)\n            \n            # Find matches\n            for i, (utterance, sim) in enumerate(zip(batch, max_sims)):\n                if sim.item() >= self.threshold:\n                    utt_idx = batch_start + i\n                    references.append({\n                        \"reference_id\": f\"SEM_{reference_id:03d}\",\n                        \"text\": utterance['text'],\n                        \"timestamp_ms\": utterance['start_ms'],\n                        \"end_ms\": utterance['end_ms'],\n                        \"matched_phrase\": \"semantic_match\",\n                        \"reference_type\": \"contextual\",\n                        \"detection_method\": \"semantic\",\n                        \"confidence\": float(sim.item()),\n                        \"context_before\": self._get_context(utterances, utt_idx, -1),\n                        \"context_after\": self._get_context(utterances, utt_idx, 1)\n                    })\n                    reference_id += 1\n        \n        # Clean up\n        del reference_embeddings, text_embeddings\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n        \n        print(f\"   Found {len(references)} semantic matches\")\n        return references\n    \n    def _classify_type(self, phrase: str) -> str:\n        \"\"\"Classify reference type based on software demonstration context.\"\"\"\n        phrase_lower = phrase.lower()\n        \n        # New Categories for UI/Software Walkthroughs\n        if any(word in phrase_lower for word in ['banner', 'widget', 'trash bag', 'button']):\n            return 'ui_element'\n        elif any(word in phrase_lower for word in ['panel', 'back-end', 'screen', 'go to']):\n            return 'interface_navigation'\n        elif any(word in phrase_lower for word in ['show you', 'look at', 'see right here']):\n            return 'visual_demonstration'\n        elif any(word in phrase_lower for word in ['part', 'point', 'step']):\n            return 'structural_transition'\n        else:\n            return 'general_reference'\n    \n    def _get_context(self, utterances: List[Dict], index: int, offset: int) -> str:\n        target_idx = index + offset\n        if 0 <= target_idx < len(utterances):\n            return utterances[target_idx]['text']\n        return \"\"\n\nprint(\"âœ… Detector class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:49:10.224622Z","iopub.execute_input":"2026-02-09T08:49:10.224976Z","iopub.status.idle":"2026-02-09T08:49:10.243010Z","shell.execute_reply.started":"2026-02-09T08:49:10.224943Z","shell.execute_reply":"2026-02-09T08:49:10.242219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 6: Run Detection\n# ====================================================================\n\nprint(\"=\"*70)\nprint(\"ðŸš€ STARTING REFERENCE DETECTION\")\nprint(\"=\"*70)\n\n# Initialize detector\ndetector = EnglishReferenceDetector(\n    phrases=CONFIG['reference_phrases'],\n    model_name=CONFIG['bert_model'] if CONFIG['use_bert'] else None,\n    use_bert=CONFIG['use_bert'],\n    threshold=CONFIG['similarity_threshold']\n)\n\n# Step 1: Regex detection (fast, high precision)\nprint(\"\\n[1/3] Regex pattern matching...\")\nregex_refs = detector.detect_with_regex(utterances)\n\n# Step 2: Semantic detection (slower, catches more)\nsemantic_refs = []\nif CONFIG['use_bert']:\n    print(\"\\n[2/3] Semantic matching...\")\n    semantic_refs = detector.detect_with_semantic(utterances)\n\n# Step 3: Merge and deduplicate\nprint(\"\\n[3/3] Merging results...\")\nall_refs = regex_refs + semantic_refs\n\n# Remove duplicates (within 2 second window)\nunique_refs = []\nseen_windows = set()\n\nfor ref in sorted(all_refs, key=lambda x: (x['timestamp_ms'], -x['confidence'])):\n    # Use 2-second windows\n    window = ref['timestamp_ms'] // 2000\n    \n    if window not in seen_windows:\n        unique_refs.append(ref)\n        seen_windows.add(window)\n\nprint(f\"\\nâœ… Detection complete!\")\nprint(f\"   Regex matches: {len(regex_refs)}\")\nprint(f\"   Semantic matches: {len(semantic_refs)}\")\nprint(f\"   Unique references: {len(unique_refs)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:49:15.290181Z","iopub.execute_input":"2026-02-09T08:49:15.290946Z","iopub.status.idle":"2026-02-09T08:49:18.365586Z","shell.execute_reply.started":"2026-02-09T08:49:15.290909Z","shell.execute_reply":"2026-02-09T08:49:18.364822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 7: Save Results\n# ====================================================================\n\nresult = {\n    \"video_id\": video_id,\n    \"video_url\": phase1_data.get('video_url', ''),\n    \"references\": unique_refs,\n    \"reference_count\": len(unique_refs),\n    \"detection_summary\": {\n        \"regex_count\": len(regex_refs),\n        \"semantic_count\": len(semantic_refs),\n        \"unique_count\": len(unique_refs),\n        \"method\": \"regex+semantic\" if CONFIG['use_bert'] else \"regex_only\"\n    },\n    \"config\": {\n        \"model\": CONFIG['bert_model'] if CONFIG['use_bert'] else None,\n        \"threshold\": CONFIG['similarity_threshold'],\n        \"phrase_count\": len(CONFIG['reference_phrases'])\n    }\n}\n\n# Save JSON\noutput_file = f\"{CONFIG['output_dir']}/phase2_references_{video_id}.json\"\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(result, f, ensure_ascii=False, indent=2)\n\n# Save simple text report\nreport_file = f\"{CONFIG['output_dir']}/phase2_report_{video_id}.txt\"\nwith open(report_file, 'w', encoding='utf-8') as f:\n    f.write(f\"Visual Reference Detection Report\\n\")\n    f.write(f\"Video ID: {video_id}\\n\")\n    f.write(f\"=\"*70 + \"\\n\\n\")\n    \n    for i, ref in enumerate(unique_refs, 1):\n        time_sec = ref['timestamp_ms'] / 1000\n        f.write(f\"[{i}] {time_sec:.1f}s - {ref['detection_method']} (conf: {ref['confidence']:.2f})\\n\")\n        f.write(f\"    {ref['text']}\\n\\n\")\n\nfile_size = os.path.getsize(output_file) / 1024\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… PHASE 2 COMPLETE!\")\nprint(\"=\"*70)\nprint(f\"ðŸ“Š Results:\")\nprint(f\"   - Total references: {len(unique_refs)}\")\nprint(f\"   - Average confidence: {np.mean([r['confidence'] for r in unique_refs]):.2f}\")\nprint(f\"\\nðŸ’¾ Files saved:\")\nprint(f\"   - JSON: {output_file} ({file_size:.1f} KB)\")\nprint(f\"   - Report: {report_file}\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:49:25.565873Z","iopub.execute_input":"2026-02-09T08:49:25.566197Z","iopub.status.idle":"2026-02-09T08:49:25.577861Z","shell.execute_reply.started":"2026-02-09T08:49:25.566163Z","shell.execute_reply":"2026-02-09T08:49:25.577049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 8: Display Sample Results\n# ====================================================================\n\nprint(\"\\nðŸ“‹ Sample References (first 10):\")\nprint(\"=\"*70)\n\nfor i, ref in enumerate(unique_refs[:10], 1):\n    time = ref['timestamp_ms'] / 1000\n    method = ref['detection_method']\n    conf = ref['confidence']\n    \n    print(f\"\\n[{i}] {time:.1f}s | {method} | confidence: {conf:.2f}\")\n    print(f\"    Type: {ref['reference_type']}\")\n    print(f\"    Text: {ref['text'][:120]}...\")\n    \n    if ref.get('context_before'):\n        print(f\"    Before: {ref['context_before'][:80]}...\")\n\nif len(unique_refs) > 10:\n    print(f\"\\n... and {len(unique_refs) - 10} more references\")\n\nprint(\"\\n\" + \"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:49:32.273867Z","iopub.execute_input":"2026-02-09T08:49:32.274170Z","iopub.status.idle":"2026-02-09T08:49:32.280927Z","shell.execute_reply.started":"2026-02-09T08:49:32.274141Z","shell.execute_reply":"2026-02-09T08:49:32.279935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 9: Statistics & Visualization\n# ====================================================================\n\nimport matplotlib.pyplot as plt\n\nif len(unique_refs) > 0:\n    # Reference timeline\n    timestamps = [r['timestamp_ms'] / 1000 for r in unique_refs]\n    confidences = [r['confidence'] for r in unique_refs]\n    \n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n    \n    # Timeline scatter plot\n    colors = ['blue' if r['detection_method'] == 'regex' else 'orange' for r in unique_refs]\n    ax1.scatter(timestamps, confidences, c=colors, alpha=0.6, s=100)\n    ax1.axhline(y=CONFIG['similarity_threshold'], color='red', linestyle='--', alpha=0.5, label='Threshold')\n    ax1.set_xlabel('Time (seconds)', fontsize=12)\n    ax1.set_ylabel('Confidence', fontsize=12)\n    ax1.set_title('Visual References Timeline', fontsize=14, fontweight='bold')\n    ax1.grid(True, alpha=0.3)\n    ax1.legend(['Threshold', 'Regex', 'Semantic'])\n    \n    # Type distribution\n    types = [r['reference_type'] for r in unique_refs]\n    type_counts = {t: types.count(t) for t in set(types)}\n    ax2.bar(type_counts.keys(), type_counts.values(), color='steelblue')\n    ax2.set_xlabel('Reference Type', fontsize=12)\n    ax2.set_ylabel('Count', fontsize=12)\n    ax2.set_title('Reference Type Distribution', fontsize=14, fontweight='bold')\n    ax2.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig(f\"{CONFIG['output_dir']}/phase2_analysis_{video_id}.png\", dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(\"ðŸ“Š Visualization saved!\")\nelse:\n    print(\"âš ï¸  No references found to visualize\")\n\n# Summary stats\nprint(f\"\\nðŸ“ˆ Statistics:\")\nprint(f\"   Average time between refs: {np.mean(np.diff(timestamps)) if len(timestamps) > 1 else 0:.1f}s\")\nprint(f\"   Reference density: {len(unique_refs) / (phase1_data['duration_ms']/1000) * 60:.1f} refs/minute\")\n\nprint(\"\\nâœ… All done! Download files from /kaggle/working/output/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:49:35.839073Z","iopub.execute_input":"2026-02-09T08:49:35.839740Z","iopub.status.idle":"2026-02-09T08:49:36.655121Z","shell.execute_reply.started":"2026-02-09T08:49:35.839708Z","shell.execute_reply":"2026-02-09T08:49:36.654524Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Phase 3**","metadata":{}},{"cell_type":"code","source":"# ====================================================================\n# CELL 1: Install Dependencies\n# ====================================================================\nprint(\"ðŸ“¦ Installing dependencies...\")\n\n# Core dependencies (always needed)\n!pip install -q opencv-python pillow imagehash\n\n# # Optional: OCR (uncomment if you want text extraction)\n# !pip install -q pytesseract\n# !apt-get install -y tesseract-ocr\n\n# Optional: Better OCR for equations/diagrams\n!pip install -q paddlepaddle paddleocr\n\n# Optional: Google Gemini for AI descriptions\n!pip install -q google-generativeai\n\nprint(\"âœ… Dependencies installed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:49:40.322006Z","iopub.execute_input":"2026-02-09T08:49:40.322397Z","iopub.status.idle":"2026-02-09T08:50:16.006442Z","shell.execute_reply.started":"2026-02-09T08:49:40.322366Z","shell.execute_reply":"2026-02-09T08:50:16.005362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 2: Import Libraries\n# ====================================================================\nimport os\nimport json\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport imagehash\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Tuple, Optional\nimport shutil\n\n# Optional imports (will fail gracefully if not installed)\ntry:\n    import pytesseract\n    TESSERACT_AVAILABLE = True\nexcept:\n    TESSERACT_AVAILABLE = False\n    print(\"âš ï¸  Tesseract not available (OCR disabled)\")\n\ntry:\n    from paddleocr import PaddleOCR\n    PADDLE_AVAILABLE = True\nexcept:\n    PADDLE_AVAILABLE = False\n    print(\"âš ï¸  PaddleOCR not available\")\n\ntry:\n    import google.generativeai as genai\n    GEMINI_AVAILABLE = True\nexcept:\n    GEMINI_AVAILABLE = False\n    print(\"âš ï¸  Google Gemini not available\")\n\nprint(\"âœ… Libraries imported\")\nprint(f\"   OCR (Tesseract): {'âœ“' if TESSERACT_AVAILABLE else 'âœ—'}\")\nprint(f\"   OCR (Paddle): {'âœ“' if PADDLE_AVAILABLE else 'âœ—'}\")\nprint(f\"   AI (Gemini): {'âœ“' if GEMINI_AVAILABLE else 'âœ—'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:50:46.883567Z","iopub.execute_input":"2026-02-09T08:50:46.885110Z","iopub.status.idle":"2026-02-09T08:50:46.891885Z","shell.execute_reply.started":"2026-02-09T08:50:46.885074Z","shell.execute_reply":"2026-02-09T08:50:46.891061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 3: Configuration\n# ====================================================================\n\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\n# Load Gemini API key from Kaggle secrets\nuser_secrets = UserSecretsClient()\ntry:\n    gemini_api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n    print(\"âœ… Gemini API key loaded from Kaggle secrets\")\nexcept:\n    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n    if gemini_api_key:\n        print(\"âœ… Gemini API key loaded from environment variable\")\n    else:\n        gemini_api_key = None\n        print(\"âš ï¸  GEMINI_API_KEY not found in Kaggle secrets\")\n\nCONFIG = {\n    # Input files\n    \"phase1_file\": \"/kaggle/working/output/transcript_XNQTWZ87K4I.json\",  # âš ï¸ UPDATE\n    \"phase2_file\": \"/kaggle/working/output/phase2_references_XNQTWZ87K4I.json\",  # âš ï¸ UPDATE\n    \n    # Frame extraction\n    \"frame_offsets_seconds\": [-1, 0, 1, 2],  # Extract at -1s, 0s, +1s, +2s from reference\n    \"max_frames_per_reference\": 4,\n    \n    # Duplicate detection\n    \"enable_dedup\": True,\n    \"perceptual_hash_threshold\": 5,  # Hamming distance (0-64, lower=stricter)\n    \n    # Quality filtering\n    \"min_brightness\": 20,   # Skip very dark frames\n    \"max_brightness\": 250,  # Skip very bright/washed out frames\n    \"min_sharpness\": 50,    # Skip blurry frames\n    \n    # OCR settings\n    \"enable_ocr\": False,  # Set True to enable\n    \"ocr_engine\": \"tesseract\",  # \"tesseract\" or \"paddle\"\n    \"ocr_languages\": \"eng\",  # Language codes\n    \n    # AI description (Gemini)\n    \"enable_ai_description\": True if gemini_api_key else False,  # Auto-enable if key exists\n    \"gemini_api_key\": gemini_api_key,  # Loaded from Kaggle secrets\n    \"gemini_model\": \"gemini-2.5-flash\",\n    \n    # Output\n    \"output_dir\": \"/kaggle/working/output\",\n    \"frames_dir\": \"/kaggle/working/frames\",\n    \"save_thumbnails\": True,  # Save smaller versions\n    \"thumbnail_size\": (640, 360)\n}\n\n# Create directories\nos.makedirs(CONFIG['output_dir'], exist_ok=True)\nos.makedirs(CONFIG['frames_dir'], exist_ok=True)\n\nprint(\"âœ… Configuration loaded\")\nprint(f\"   Frame offsets: {CONFIG['frame_offsets_seconds']}\")\nprint(f\"   Deduplication: {'Enabled' if CONFIG['enable_dedup'] else 'Disabled'}\")\nprint(f\"   OCR: {'Enabled (' + CONFIG['ocr_engine'] + ')' if CONFIG['enable_ocr'] else 'Disabled'}\")\nprint(f\"   AI Description: {'Enabled' if CONFIG['enable_ai_description'] else 'Disabled'}\")\n\nif CONFIG['enable_ai_description']:\n    print(f\"   ðŸ”‘ API key ready (length: {len(gemini_api_key)} chars)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:50:50.520582Z","iopub.execute_input":"2026-02-09T08:50:50.520916Z","iopub.status.idle":"2026-02-09T08:50:50.605350Z","shell.execute_reply.started":"2026-02-09T08:50:50.520889Z","shell.execute_reply":"2026-02-09T08:50:50.604393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 4: Load Phase 1 & 2 Data\n# ====================================================================\n\nprint(\"ðŸ“‚ Loading previous phase outputs...\")\n\n# Load Phase 1 (transcript)\nwith open(CONFIG['phase1_file'], 'r', encoding='utf-8') as f:\n    phase1_data = json.load(f)\n\n# Load Phase 2 (references)\nwith open(CONFIG['phase2_file'], 'r', encoding='utf-8') as f:\n    phase2_data = json.load(f)\n\nvideo_path = phase1_data['video_path']\nvideo_id = phase1_data['video_id']\nreferences = phase2_data['references']\n\nprint(f\"âœ… Data loaded:\")\nprint(f\"   Video: {Path(video_path).name}\")\nprint(f\"   Video ID: {video_id}\")\nprint(f\"   References: {len(references)}\")\n\nif len(references) == 0:\n    print(\"\\nâš ï¸  No references found! Phase 2 didn't detect any visual references.\")\n    print(\"   Try lowering the similarity threshold in Phase 2.\")\nelse:\n    print(f\"\\nðŸ“ First reference: {references[0]['timestamp_ms']/1000:.1f}s\")\n    print(f\"   Text: {references[0]['text'][:100]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:50:55.563719Z","iopub.execute_input":"2026-02-09T08:50:55.564379Z","iopub.status.idle":"2026-02-09T08:50:55.574079Z","shell.execute_reply.started":"2026-02-09T08:50:55.564345Z","shell.execute_reply":"2026-02-09T08:50:55.573335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 5: Frame Extraction Functions\n# ====================================================================\n\nclass FrameExtractor:\n    \"\"\"Extract frames from video at specified timestamps.\"\"\"\n    \n    def __init__(self, video_path: str):\n        self.video_path = video_path\n        self.cap = cv2.VideoCapture(video_path)\n        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n        if not self.cap.isOpened():\n            raise ValueError(f\"Could not open video: {video_path}\")\n    \n    def extract_frame_at_timestamp(self, timestamp_ms: int, output_path: str) -> bool:\n        \"\"\"Extract single frame at timestamp.\"\"\"\n        # Convert timestamp to frame number\n        frame_num = int((timestamp_ms / 1000.0) * self.fps)\n        \n        # Validate frame number\n        if frame_num < 0 or frame_num >= self.total_frames:\n            return False\n        \n        # Seek to frame\n        self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n        ret, frame = self.cap.read()\n        \n        if not ret:\n            return False\n        \n        # Save frame\n        cv2.imwrite(output_path, frame)\n        return True\n    \n    def extract_frames_with_offsets(self, base_timestamp_ms: int, \n                                    offsets_seconds: List[float],\n                                    output_dir: str,\n                                    base_name: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract multiple frames around a timestamp.\"\"\"\n        frames = []\n        \n        for i, offset in enumerate(offsets_seconds):\n            timestamp_ms = base_timestamp_ms + int(offset * 1000)\n            output_path = os.path.join(output_dir, f\"{base_name}_offset_{offset:+.1f}s.jpg\")\n            \n            success = self.extract_frame_at_timestamp(timestamp_ms, output_path)\n            \n            if success:\n                frames.append({\n                    \"path\": output_path,\n                    \"timestamp_ms\": timestamp_ms,\n                    \"offset_seconds\": offset\n                })\n        \n        return frames\n    \n    def close(self):\n        \"\"\"Release video capture.\"\"\"\n        self.cap.release()\n\nprint(\"âœ… Frame extractor defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:51:01.450679Z","iopub.execute_input":"2026-02-09T08:51:01.451015Z","iopub.status.idle":"2026-02-09T08:51:01.460730Z","shell.execute_reply.started":"2026-02-09T08:51:01.450982Z","shell.execute_reply":"2026-02-09T08:51:01.459976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 6: Quality Assessment & Deduplication\n# ====================================================================\n\ndef assess_frame_quality(image_path: str) -> Dict[str, Any]:\n    \"\"\"Assess frame quality (brightness, sharpness).\"\"\"\n    img = cv2.imread(image_path)\n    \n    # Convert to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # Brightness (mean pixel value)\n    brightness = np.mean(gray)\n    \n    # Sharpness (Laplacian variance)\n    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n    sharpness = laplacian.var()\n    \n    # Overall quality score (0-1)\n    brightness_score = 1 - abs(brightness - 127.5) / 127.5  # Prefer mid-range\n    sharpness_score = min(sharpness / 500, 1.0)  # Normalize\n    quality_score = (brightness_score * 0.3 + sharpness_score * 0.7)\n    \n    return {\n        \"brightness\": float(brightness),\n        \"sharpness\": float(sharpness),\n        \"quality_score\": float(quality_score),\n        \"is_good_quality\": (\n            CONFIG['min_brightness'] < brightness < CONFIG['max_brightness'] and\n            sharpness > CONFIG['min_sharpness']\n        )\n    }\n\ndef calculate_perceptual_hash(image_path: str) -> str:\n    \"\"\"Calculate perceptual hash for duplicate detection.\"\"\"\n    img = Image.open(image_path)\n    phash = imagehash.phash(img)\n    return str(phash)\n\ndef is_duplicate(phash: str, seen_hashes: List[str], threshold: int = 5) -> bool:\n    \"\"\"Check if frame is duplicate based on perceptual hash.\"\"\"\n    for seen_hash in seen_hashes:\n        hash1 = imagehash.hex_to_hash(phash)\n        hash2 = imagehash.hex_to_hash(seen_hash)\n        distance = hash1 - hash2\n        \n        if distance <= threshold:\n            return True\n    \n    return False\n\ndef create_thumbnail(image_path: str, output_path: str, size: Tuple[int, int]):\n    \"\"\"Create thumbnail of image.\"\"\"\n    img = Image.open(image_path)\n    img.thumbnail(size, Image.Resampling.LANCZOS)\n    img.save(output_path, \"JPEG\", quality=85)\n\nprint(\"âœ… Quality assessment functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:51:06.283118Z","iopub.execute_input":"2026-02-09T08:51:06.283467Z","iopub.status.idle":"2026-02-09T08:51:06.293108Z","shell.execute_reply.started":"2026-02-09T08:51:06.283436Z","shell.execute_reply":"2026-02-09T08:51:06.292244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 7: OCR Functions (Optional)\n# ====================================================================\n\ndef extract_text_tesseract(image_path: str, lang: str = \"eng\") -> Dict[str, Any]:\n    \"\"\"Extract text using Tesseract OCR.\"\"\"\n    if not TESSERACT_AVAILABLE:\n        return {\"text\": \"\", \"error\": \"Tesseract not available\"}\n    \n    try:\n        img = Image.open(image_path)\n        text = pytesseract.image_to_string(img, lang=lang)\n        confidence = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n        \n        # Calculate average confidence\n        confs = [c for c in confidence['conf'] if c != -1]\n        avg_conf = np.mean(confs) if confs else 0\n        \n        return {\n            \"text\": text.strip(),\n            \"confidence\": float(avg_conf),\n            \"engine\": \"tesseract\"\n        }\n    except Exception as e:\n        return {\"text\": \"\", \"error\": str(e)}\n\ndef extract_text_paddle(image_path: str, lang: str = \"en\") -> Dict[str, Any]:\n    \"\"\"Extract text using PaddleOCR (better for diagrams/equations).\"\"\"\n    if not PADDLE_AVAILABLE:\n        return {\"text\": \"\", \"error\": \"PaddleOCR not available\"}\n    \n    try:\n        ocr = PaddleOCR(use_angle_cls=True, lang=lang, show_log=False)\n        result = ocr.ocr(image_path, cls=True)\n        \n        # Extract text and confidence\n        texts = []\n        confidences = []\n        \n        if result and result[0]:\n            for line in result[0]:\n                texts.append(line[1][0])\n                confidences.append(line[1][1])\n        \n        return {\n            \"text\": \"\\n\".join(texts),\n            \"confidence\": float(np.mean(confidences)) if confidences else 0,\n            \"engine\": \"paddle\"\n        }\n    except Exception as e:\n        return {\"text\": \"\", \"error\": str(e)}\n\ndef extract_text_from_frame(image_path: str) -> Dict[str, Any]:\n    \"\"\"Extract text using configured OCR engine.\"\"\"\n    if not CONFIG['enable_ocr']:\n        return None\n    \n    if CONFIG['ocr_engine'] == 'paddle':\n        return extract_text_paddle(image_path, CONFIG['ocr_languages'])\n    else:\n        return extract_text_tesseract(image_path, CONFIG['ocr_languages'])\n\nprint(\"âœ… OCR functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:51:13.440916Z","iopub.execute_input":"2026-02-09T08:51:13.441700Z","iopub.status.idle":"2026-02-09T08:51:13.451649Z","shell.execute_reply.started":"2026-02-09T08:51:13.441645Z","shell.execute_reply":"2026-02-09T08:51:13.450615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 8: AI Description (Gemini - Optional)\n# ====================================================================\n\ndef generate_ai_description(image_path: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Generate AI description using Google Gemini.\"\"\"\n    if not CONFIG['enable_ai_description']:\n        return None\n    \n    if not GEMINI_AVAILABLE:\n        return {\"description\": \"\", \"error\": \"Gemini not available\"}\n    \n    try:\n        # Configure API\n        api_key = CONFIG['gemini_api_key'] or os.getenv('GEMINI_API_KEY')\n        if not api_key:\n            return {\"description\": \"\", \"error\": \"No API key provided\"}\n        \n        genai.configure(api_key=api_key)\n        model = genai.GenerativeModel(CONFIG['gemini_model'])\n        \n        # Load image\n        img = Image.open(image_path)\n        \n        # Generate description\n        prompt = \"\"\"\n        Analyze this video frame from a software compliance walkthrough and provide:\n\n        1. A concise description of the UI state (e.g., \"Showing the cookie banner settings\" or \"Inside the admin dashboard\").\n        2. Identify specific interactive elements visible: (e.g., \"Accept/Decline buttons\", \"Consent tracking table\", or \"The trash bag widget\").\n        3. Determine the demonstration context: Is the speaker explaining a concept, showing a specific setting, or navigating to a new menu?\n\n        Format as JSON: \n        {\n          \"description\": \"...\", \n          \"visual_elements\": [\"element1\", \"element2\"],\n          \"type\": \"UI_Walkthrough/Navigation/Data_Table/Settings\", \n          \"concept\": \"GDPR Compliance / Cookie Management\"\n        }\n        \"\"\"\n        \n        response = model.generate_content([prompt, img])\n        \n        # Try to parse JSON response\n        try:\n            result = json.loads(response.text)\n        except:\n            result = {\n                \"description\": response.text,\n                \"type\": \"unknown\",\n                \"concept\": \"\"\n            }\n        \n        return result\n        \n    except Exception as e:\n        return {\"description\": \"\", \"error\": str(e)}\n\nprint(\"âœ… AI description function defined\")\nif CONFIG['enable_ai_description'] and not CONFIG['gemini_api_key']:\n    print(\"âš ï¸  To use Gemini, set CONFIG['gemini_api_key'] or GEMINI_API_KEY env var\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:51:18.003968Z","iopub.execute_input":"2026-02-09T08:51:18.004306Z","iopub.status.idle":"2026-02-09T08:51:18.012407Z","shell.execute_reply.started":"2026-02-09T08:51:18.004263Z","shell.execute_reply":"2026-02-09T08:51:18.011543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 9: Main Processing Loop\n# ====================================================================\n\nprint(\"=\"*70)\nprint(\"ðŸš€ STARTING PHASE 3: VISUAL EXTRACTION\")\nprint(\"=\"*70)\n\n# Initialize extractor\nextractor = FrameExtractor(video_path)\n\nall_frames = []\nseen_hashes = []\ntotal_extracted = 0\nduplicates_skipped = 0\nlow_quality_skipped = 0\n\nprint(f\"\\nProcessing {len(references)} references...\\n\")\n\nfor ref_idx, reference in enumerate(references, 1):\n    ref_id = reference['reference_id']\n    timestamp_ms = reference['timestamp_ms']\n    \n    print(f\"[{ref_idx}/{len(references)}] {ref_id} @ {timestamp_ms/1000:.1f}s\")\n    print(f\"  Text: {reference['text'][:80]}...\")\n    \n    # Create reference directory\n    ref_dir = os.path.join(CONFIG['frames_dir'], ref_id)\n    os.makedirs(ref_dir, exist_ok=True)\n    \n    # Extract frames with offsets\n    frames = extractor.extract_frames_with_offsets(\n        timestamp_ms,\n        CONFIG['frame_offsets_seconds'],\n        ref_dir,\n        ref_id\n    )\n    \n    print(f\"  Extracted {len(frames)} frames\")\n    \n    # Process each frame\n    for frame in frames:\n        total_extracted += 1\n        frame_path = frame['path']\n        \n        # Quality assessment\n        quality = assess_frame_quality(frame_path)\n        \n        if not quality['is_good_quality']:\n            print(f\"    âš ï¸  Low quality (brightness={quality['brightness']:.0f}, sharpness={quality['sharpness']:.0f})\")\n            low_quality_skipped += 1\n            continue\n        \n        # Duplicate detection\n        phash = calculate_perceptual_hash(frame_path)\n        is_dup = is_duplicate(phash, seen_hashes, CONFIG['perceptual_hash_threshold']) if CONFIG['enable_dedup'] else False\n        \n        if is_dup:\n            print(f\"    ðŸ”„ Duplicate detected (skipping)\")\n            duplicates_skipped += 1\n            continue\n        \n        seen_hashes.append(phash)\n        \n        # Create thumbnail\n        if CONFIG['save_thumbnails']:\n            thumb_path = frame_path.replace('.jpg', '_thumb.jpg')\n            create_thumbnail(frame_path, thumb_path, CONFIG['thumbnail_size'])\n        else:\n            thumb_path = None\n        \n        # OCR extraction\n        ocr_data = extract_text_from_frame(frame_path)\n        if ocr_data and ocr_data.get('text'):\n            print(f\"    ðŸ“ OCR: {ocr_data['text'][:60]}...\")\n        \n        # AI description\n        ai_desc = generate_ai_description(frame_path)\n        if ai_desc and ai_desc.get('description'):\n            print(f\"    ðŸ¤– AI: {ai_desc['description'][:60]}...\")\n        \n        # Store frame info\n        frame_info = {\n            \"frame_id\": f\"{ref_id}_F{len(all_frames)}\",\n            \"reference_id\": ref_id,\n            \"reference_text\": reference['text'],\n            \"timestamp_ms\": frame['timestamp_ms'],\n            \"offset_seconds\": frame['offset_seconds'],\n            \"frame_path\": frame_path,\n            \"thumbnail_path\": thumb_path,\n            \"perceptual_hash\": phash,\n            \"quality\": quality,\n            \"ocr_data\": ocr_data,\n            \"ai_description\": ai_desc\n        }\n        \n        all_frames.append(frame_info)\n        print(f\"    âœ… Processed (quality={quality['quality_score']:.2f})\")\n    \n    print()\n\n# Cleanup\nextractor.close()\n\nprint(\"=\"*70)\nprint(\"âœ… EXTRACTION COMPLETE\")\nprint(\"=\"*70)\nprint(f\"ðŸ“Š Statistics:\")\nprint(f\"   Total extracted: {total_extracted}\")\nprint(f\"   Duplicates skipped: {duplicates_skipped}\")\nprint(f\"   Low quality skipped: {low_quality_skipped}\")\nprint(f\"   Unique frames kept: {len(all_frames)}\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:51:24.271063Z","iopub.execute_input":"2026-02-09T08:51:24.271419Z","iopub.status.idle":"2026-02-09T08:53:02.305514Z","shell.execute_reply.started":"2026-02-09T08:51:24.271387Z","shell.execute_reply":"2026-02-09T08:53:02.304756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 10: Save Results\n# ====================================================================\n\ndef convert_to_serializable(obj):\n    \"\"\"Convert numpy/non-serializable types to JSON-compatible types.\"\"\"\n    import numpy as np\n    \n    if isinstance(obj, dict):\n        return {key: convert_to_serializable(value) for key, value in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_serializable(item) for item in obj]\n    elif isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.bool_):\n        return bool(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n\n# Convert result to JSON-serializable format\nresult = {\n    \"video_id\": video_id,\n    \"video_path\": video_path,\n    \"frames\": convert_to_serializable(all_frames),\n    \"frame_count\": len(all_frames),\n    \"statistics\": {\n        \"total_extracted\": int(total_extracted),\n        \"duplicates_skipped\": int(duplicates_skipped),\n        \"low_quality_skipped\": int(low_quality_skipped),\n        \"unique_frames\": len(all_frames)\n    },\n    \"config\": {\n        \"frame_offsets\": CONFIG['frame_offsets_seconds'],\n        \"dedup_enabled\": bool(CONFIG['enable_dedup']),\n        \"ocr_enabled\": bool(CONFIG['enable_ocr']),\n        \"ai_enabled\": bool(CONFIG['enable_ai_description'])\n    }\n}\n\n# Save JSON\noutput_file = f\"{CONFIG['output_dir']}/phase3_frames_{video_id}.json\"\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(result, f, ensure_ascii=False, indent=2)\n\n# Save text report\nreport_file = f\"{CONFIG['output_dir']}/phase3_report_{video_id}.txt\"\nwith open(report_file, 'w', encoding='utf-8') as f:\n    f.write(\"Visual Frame Extraction Report\\n\")\n    f.write(f\"Video ID: {video_id}\\n\")\n    f.write(\"=\"*70 + \"\\n\\n\")\n    \n    for i, frame in enumerate(all_frames, 1):\n        f.write(f\"[{i}] Frame ID: {frame['frame_id']}\\n\")\n        f.write(f\"    Time: {frame['timestamp_ms']/1000:.1f}s (offset: {frame['offset_seconds']:+.1f}s)\\n\")\n        f.write(f\"    Reference: {frame['reference_text'][:80]}...\\n\")\n        f.write(f\"    Quality: {frame['quality']['quality_score']:.2f}\\n\")\n        f.write(f\"    Path: {frame['frame_path']}\\n\")\n        \n        if frame.get('ocr_data') and frame['ocr_data'].get('text'):\n            f.write(f\"    OCR: {frame['ocr_data']['text'][:100]}...\\n\")\n        \n        if frame.get('ai_description') and frame['ai_description'].get('description'):\n            f.write(f\"    AI: {frame['ai_description']['description'][:100]}...\\n\")\n        \n        f.write(\"\\n\")\n\nfile_size = os.path.getsize(output_file) / 1024\n\nprint(\"\\nðŸ’¾ Files saved:\")\nprint(f\"   JSON: {output_file} ({file_size:.1f} KB)\")\nprint(f\"   Report: {report_file}\")\nprint(f\"   Frames: {CONFIG['frames_dir']}/\")\nprint(\"\\nâœ… Phase 3 complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:53:11.988398Z","iopub.execute_input":"2026-02-09T08:53:11.988728Z","iopub.status.idle":"2026-02-09T08:53:12.003156Z","shell.execute_reply.started":"2026-02-09T08:53:11.988696Z","shell.execute_reply":"2026-02-09T08:53:12.002206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 11: Display Sample Frames\n# ====================================================================\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nif len(all_frames) > 0:\n    # Show first 6 frames\n    num_samples = min(6, len(all_frames))\n    \n    fig = plt.figure(figsize=(16, 8))\n    gs = gridspec.GridSpec(2, 3, hspace=0.3, wspace=0.2)\n    \n    for i in range(num_samples):\n        frame = all_frames[i]\n        \n        ax = fig.add_subplot(gs[i])\n        \n        # Load and display image\n        img = cv2.imread(frame['frame_path'])\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        ax.imshow(img_rgb)\n        \n        # Title with info\n        time = frame['timestamp_ms'] / 1000\n        quality = frame['quality']['quality_score']\n        title = f\"{frame['frame_id']}\\n{time:.1f}s | Q={quality:.2f}\"\n        ax.set_title(title, fontsize=10)\n        ax.axis('off')\n    \n    plt.suptitle(f\"Sample Extracted Frames ({num_samples}/{len(all_frames)})\", \n                 fontsize=14, fontweight='bold')\n    plt.savefig(f\"{CONFIG['output_dir']}/phase3_samples_{video_id}.png\", \n                dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"\\nðŸ“¸ Sample visualization saved!\")\nelse:\n    print(\"âš ï¸  No frames to display\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:53:20.912458Z","iopub.execute_input":"2026-02-09T08:53:20.913190Z","iopub.status.idle":"2026-02-09T08:53:21.969441Z","shell.execute_reply.started":"2026-02-09T08:53:20.913158Z","shell.execute_reply":"2026-02-09T08:53:21.968659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 12: Timeline Visualization\n# ====================================================================\n\nif len(all_frames) > 0:\n    fig, ax = plt.subplots(figsize=(16, 4))\n    \n    # Plot frames on timeline\n    timestamps = [f['timestamp_ms'] / 1000 for f in all_frames]\n    qualities = [f['quality']['quality_score'] for f in all_frames]\n    \n    scatter = ax.scatter(timestamps, qualities, \n                        s=100, alpha=0.6, c=qualities, \n                        cmap='viridis', edgecolors='black', linewidth=0.5)\n    \n    # Add reference markers\n    ref_times = [r['timestamp_ms'] / 1000 for r in references]\n    for t in ref_times:\n        ax.axvline(x=t, color='red', alpha=0.3, linestyle='--', linewidth=1)\n    \n    ax.set_xlabel('Time (seconds)', fontsize=12)\n    ax.set_ylabel('Frame Quality Score', fontsize=12)\n    ax.set_title('Extracted Frames Timeline\\n(Red lines = reference timestamps)', \n                 fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n    \n    plt.colorbar(scatter, label='Quality Score', ax=ax)\n    plt.tight_layout()\n    plt.savefig(f\"{CONFIG['output_dir']}/phase3_timeline_{video_id}.png\", \n                dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(\"ðŸ“Š Timeline visualization saved!\")\n\nprint(\"\\nâœ… All visualizations complete!\")\nprint(f\"\\nðŸ“ Download all files from: {CONFIG['output_dir']}/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:53:25.662997Z","iopub.execute_input":"2026-02-09T08:53:25.663341Z","iopub.status.idle":"2026-02-09T08:53:26.189132Z","shell.execute_reply.started":"2026-02-09T08:53:25.663303Z","shell.execute_reply":"2026-02-09T08:53:26.188194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Phase 4**","metadata":{}},{"cell_type":"code","source":"# ====================================================================\n# CELL 1: Install Dependencies\n# ====================================================================\nprint(\"ðŸ“¦ Installing dependencies...\")\n\n# Core dependencies\n!pip install -q transformers torch pillow scikit-learn numpy\n\n# CLIP model\n!pip install -q sentence-transformers  # Includes CLIP\n\nprint(\"âœ… Dependencies installed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:02:32.894547Z","iopub.execute_input":"2026-02-09T07:02:32.895111Z","iopub.status.idle":"2026-02-09T07:02:39.993827Z","shell.execute_reply.started":"2026-02-09T07:02:32.895080Z","shell.execute_reply":"2026-02-09T07:02:39.993014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 2: Import Libraries\n# ====================================================================\nimport os\nimport json\nimport numpy as np\nfrom typing import Dict, List, Any, Tuple\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport torch\nfrom PIL import Image\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport gc\n\nprint(f\"ðŸ–¥ï¸  Device: {'CUDA (GPU)' if torch.cuda.is_available() else 'CPU'}\")\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    gc.collect()\n\nprint(\"âœ… Libraries imported\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:02:47.904590Z","iopub.execute_input":"2026-02-09T07:02:47.904909Z","iopub.status.idle":"2026-02-09T07:02:48.557599Z","shell.execute_reply.started":"2026-02-09T07:02:47.904876Z","shell.execute_reply":"2026-02-09T07:02:48.556837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 3: Configuration\n# ====================================================================\n\nCONFIG = {\n    # Input files from previous phases\n    \"phase1_file\": \"/kaggle/working/output/transcript_XNQTWZ87K4I.json\",  # âš ï¸ UPDATE\n    \"phase2_file\": \"/kaggle/working/output/phase2_references_XNQTWZ87K4I.json\",  # âš ï¸ UPDATE\n    \"phase3_file\": \"/kaggle/working/output/phase3_frames_XNQTWZ87K4I.json\",  # âš ï¸ UPDATE\n    \n    # CLIP model for visual similarity\n    \"clip_model\": \"clip-ViT-B-32\",  # Options: clip-ViT-B-32, clip-ViT-L-14 (larger, better)\n    \n    # Clustering parameters (DBSCAN)\n    \"clustering\": {\n        \"eps\": 0.3,  # Distance threshold (0.2-0.4, lower=stricter)\n        \"min_samples\": 1,  # Minimum frames to form cluster\n        \"metric\": \"cosine\"  # Cosine distance for CLIP embeddings\n    },\n    \n    # Representative selection\n    \"select_best_by\": \"quality\",  # \"quality\" or \"central\" (most similar to cluster center)\n    \n    # Output\n    \"output_dir\": \"/kaggle/working/output\",\n    \"final_output_file\": \"enriched_transcript.json\",\n    \"save_intermediate\": True\n}\n\nos.makedirs(CONFIG['output_dir'], exist_ok=True)\n\nprint(\"âœ… Configuration loaded\")\nprint(f\"   CLIP model: {CONFIG['clip_model']}\")\nprint(f\"   DBSCAN eps: {CONFIG['clustering']['eps']}\")\nprint(f\"   Min samples: {CONFIG['clustering']['min_samples']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:02:52.680642Z","iopub.execute_input":"2026-02-09T07:02:52.680949Z","iopub.status.idle":"2026-02-09T07:02:52.686970Z","shell.execute_reply.started":"2026-02-09T07:02:52.680922Z","shell.execute_reply":"2026-02-09T07:02:52.686173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 4: Load All Previous Phase Data\n# ====================================================================\n\nprint(\"ðŸ“‚ Loading Phase 1, 2, 3 outputs...\\n\")\n\n# Load Phase 1 (transcript)\nwith open(CONFIG['phase1_file'], 'r', encoding='utf-8') as f:\n    phase1_data = json.load(f)\nprint(f\"âœ… Phase 1: {len(phase1_data['utterances'])} utterances\")\n\n# Load Phase 2 (references)\nwith open(CONFIG['phase2_file'], 'r', encoding='utf-8') as f:\n    phase2_data = json.load(f)\nprint(f\"âœ… Phase 2: {len(phase2_data['references'])} references\")\n\n# Load Phase 3 (frames)\nwith open(CONFIG['phase3_file'], 'r', encoding='utf-8') as f:\n    phase3_data = json.load(f)\nprint(f\"âœ… Phase 3: {len(phase3_data['frames'])} frames\")\n\nvideo_id = phase1_data['video_id']\nframes = phase3_data['frames']\n\n# Filter valid frames (with existing paths)\nvalid_frames = [\n    f for f in frames \n    if f.get('frame_path') and os.path.exists(f['frame_path'])\n]\n\nprint(f\"\\nðŸ“Š Summary:\")\nprint(f\"   Video ID: {video_id}\")\nprint(f\"   Total frames: {len(frames)}\")\nprint(f\"   Valid frames: {len(valid_frames)}\")\n\nif len(valid_frames) == 0:\n    print(\"\\nâš ï¸  No valid frames found! Check Phase 3 output.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:02:56.320545Z","iopub.execute_input":"2026-02-09T07:02:56.320847Z","iopub.status.idle":"2026-02-09T07:02:56.330952Z","shell.execute_reply.started":"2026-02-09T07:02:56.320820Z","shell.execute_reply":"2026-02-09T07:02:56.330365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 5: CLIP Embedding Functions\n# ====================================================================\n\nclass CLIPEmbedder:\n    \"\"\"Generate CLIP embeddings for images.\"\"\"\n    \n    def __init__(self, model_name: str = \"clip-ViT-B-32\"):\n        print(f\"ðŸ¤– Loading CLIP model: {model_name}...\")\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = SentenceTransformer(model_name, device=device)\n        self.device = device\n        print(f\"âœ… CLIP model loaded on {device}\")\n    \n    def embed_image(self, image_path: str) -> np.ndarray:\n        \"\"\"Generate embedding for single image.\"\"\"\n        try:\n            img = Image.open(image_path).convert('RGB')\n            embedding = self.model.encode(img, convert_to_numpy=True)\n            return embedding\n        except Exception as e:\n            print(f\"âš ï¸  Error encoding {image_path}: {e}\")\n            return None\n    \n    def embed_images_batch(self, image_paths: List[str], batch_size: int = 8) -> np.ndarray:\n        \"\"\"Generate embeddings for multiple images in batches.\"\"\"\n        embeddings = []\n        \n        print(f\"   Encoding {len(image_paths)} images in batches of {batch_size}...\")\n        \n        for i in range(0, len(image_paths), batch_size):\n            batch_paths = image_paths[i:i+batch_size]\n            \n            # Load images\n            images = []\n            for path in batch_paths:\n                try:\n                    img = Image.open(path).convert('RGB')\n                    images.append(img)\n                except Exception as e:\n                    print(f\"âš ï¸  Skipping {path}: {e}\")\n                    images.append(None)\n            \n            # Encode batch\n            valid_images = [img for img in images if img is not None]\n            if valid_images:\n                batch_embeddings = self.model.encode(\n                    valid_images,\n                    convert_to_numpy=True,\n                    show_progress_bar=False\n                )\n                embeddings.extend(batch_embeddings)\n            \n            if (i // batch_size + 1) % 10 == 0:\n                print(f\"   Progress: {i+len(batch_paths)}/{len(image_paths)}\")\n        \n        return np.array(embeddings)\n\nprint(\"âœ… CLIP embedder class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:03:00.923316Z","iopub.execute_input":"2026-02-09T07:03:00.923831Z","iopub.status.idle":"2026-02-09T07:03:00.932852Z","shell.execute_reply.started":"2026-02-09T07:03:00.923801Z","shell.execute_reply":"2026-02-09T07:03:00.932216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 6: Clustering & Selection Functions\n# ====================================================================\n\ndef cluster_embeddings(embeddings: np.ndarray, eps: float = 0.3, \n                      min_samples: int = 1, metric: str = 'cosine') -> np.ndarray:\n    \"\"\"Cluster embeddings using DBSCAN.\"\"\"\n    print(f\"\\nðŸ” Clustering {len(embeddings)} embeddings...\")\n    print(f\"   eps={eps}, min_samples={min_samples}, metric={metric}\")\n    \n    # DBSCAN clustering\n    clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n    labels = clustering.fit_predict(embeddings)\n    \n    # Count clusters\n    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise = list(labels).count(-1)\n    \n    print(f\"âœ… Found {n_clusters} clusters + {n_noise} noise points\")\n    \n    return labels\n\ndef select_best_representative(frames: List[Dict], frame_indices: List[int],\n                              embeddings: np.ndarray, method: str = \"quality\") -> int:\n    \"\"\"Select best representative frame from cluster.\"\"\"\n    if len(frame_indices) == 1:\n        return frame_indices[0]\n    \n    if method == \"quality\":\n        # Select frame with highest quality score\n        qualities = [\n            frames[idx].get('quality', {}).get('quality_score', 0)\n            for idx in frame_indices\n        ]\n        best_local_idx = np.argmax(qualities)\n        return frame_indices[best_local_idx]\n    \n    elif method == \"central\":\n        # Select frame closest to cluster centroid\n        cluster_embeddings = embeddings[frame_indices]\n        centroid = cluster_embeddings.mean(axis=0)\n        \n        # Find closest to centroid\n        similarities = cosine_similarity([centroid], cluster_embeddings)[0]\n        best_local_idx = np.argmax(similarities)\n        return frame_indices[best_local_idx]\n    \n    return frame_indices[0]\n\nprint(\"âœ… Clustering functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:03:04.589777Z","iopub.execute_input":"2026-02-09T07:03:04.590066Z","iopub.status.idle":"2026-02-09T07:03:04.598631Z","shell.execute_reply.started":"2026-02-09T07:03:04.590039Z","shell.execute_reply":"2026-02-09T07:03:04.597667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 7: Generate CLIP Embeddings\n# ====================================================================\n\nif len(valid_frames) > 0:\n    print(\"=\"*70)\n    print(\"ðŸš€ GENERATING CLIP EMBEDDINGS\")\n    print(\"=\"*70)\n    \n    # Initialize embedder\n    embedder = CLIPEmbedder(CONFIG['clip_model'])\n    \n    # Extract frame paths\n    frame_paths = [f['frame_path'] for f in valid_frames]\n    \n    # Generate embeddings\n    embeddings = embedder.embed_images_batch(frame_paths, batch_size=8)\n    \n    print(f\"\\nâœ… Generated {len(embeddings)} embeddings\")\n    print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n    \n    # Clean up\n    del embedder\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n    \nelse:\n    print(\"âš ï¸  No valid frames to embed\")\n    embeddings = np.array([])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:03:09.097323Z","iopub.execute_input":"2026-02-09T07:03:09.097963Z","iopub.status.idle":"2026-02-09T07:03:11.100606Z","shell.execute_reply.started":"2026-02-09T07:03:09.097934Z","shell.execute_reply":"2026-02-09T07:03:11.099966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 8: Cluster Similar Frames\n# ====================================================================\n\nif len(embeddings) > 0:\n    print(\"\\n\" + \"=\"*70)\n    print(\"ðŸ” CLUSTERING VISUAL FRAMES\")\n    print(\"=\"*70)\n    \n    # Cluster embeddings\n    labels = cluster_embeddings(\n        embeddings,\n        eps=CONFIG['clustering']['eps'],\n        min_samples=CONFIG['clustering']['min_samples'],\n        metric=CONFIG['clustering']['metric']\n    )\n    \n    # Organize clusters\n    clusters = defaultdict(list)\n    frame_to_cluster = {}\n    \n    for i, (frame, label) in enumerate(zip(valid_frames, labels)):\n        frame_id = frame['frame_id']\n        \n        if label == -1:\n            # Noise point - unique visual\n            cluster_id = f\"VISUAL_{i:03d}_SINGLE\"\n        else:\n            # Part of cluster\n            cluster_id = f\"VISUAL_{label:03d}\"\n        \n        clusters[cluster_id].append(i)\n        frame_to_cluster[frame_id] = cluster_id\n    \n    print(f\"\\nðŸ“Š Clustering results:\")\n    print(f\"   Total clusters: {len(clusters)}\")\n    print(f\"   Average cluster size: {np.mean([len(v) for v in clusters.values()]):.1f}\")\n    print(f\"   Largest cluster: {max(len(v) for v in clusters.values())} frames\")\n    \n    # Select representatives\n    print(f\"\\nðŸŽ¯ Selecting best representatives (method: {CONFIG['select_best_by']})...\")\n    cluster_representatives = {}\n    \n    for cluster_id, frame_indices in clusters.items():\n        best_idx = select_best_representative(\n            valid_frames,\n            frame_indices,\n            embeddings,\n            method=CONFIG['select_best_by']\n        )\n        cluster_representatives[cluster_id] = best_idx\n    \n    print(f\"âœ… Selected {len(cluster_representatives)} representatives\")\n    \nelse:\n    print(\"âš ï¸  No embeddings to cluster\")\n    clusters = {}\n    frame_to_cluster = {}\n    cluster_representatives = {}\n    labels = np.array([])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:03:21.619735Z","iopub.execute_input":"2026-02-09T07:03:21.620013Z","iopub.status.idle":"2026-02-09T07:03:21.630450Z","shell.execute_reply.started":"2026-02-09T07:03:21.619988Z","shell.execute_reply":"2026-02-09T07:03:21.629706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 9: Create Visual Database\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸŽ¨ CREATING VISUAL DATABASE\")\nprint(\"=\"*70)\n\nvisuals = {}\n\nfor cluster_id, frame_indices in clusters.items():\n    if not frame_indices:\n        continue\n    \n    # Get representative frame\n    rep_idx = cluster_representatives.get(cluster_id, frame_indices[0])\n    rep_frame = valid_frames[rep_idx]\n    \n    # Collect all timestamps where this visual appears\n    timestamps = [valid_frames[idx]['timestamp_ms'] for idx in frame_indices]\n    \n    # Build visual entry\n    visual_entry = {\n        \"representative_frame\": rep_frame['frame_path'],\n        \"thumbnail\": rep_frame.get('thumbnail_path'),\n        \"appears_at_ms\": sorted(timestamps),\n        \"appearance_count\": len(timestamps),\n        \"quality\": rep_frame.get('quality', {}),\n        \"frame_ids\": [valid_frames[idx]['frame_id'] for idx in frame_indices]\n    }\n    \n    # Add OCR data if available\n    ocr_data = rep_frame.get('ocr_data')\n    if ocr_data and ocr_data.get('text'):\n        visual_entry['ocr_text'] = ocr_data['text']\n        visual_entry['ocr_confidence'] = ocr_data.get('confidence', 0)\n    \n    # Add AI description if available\n    ai_desc = rep_frame.get('ai_description')\n    if ai_desc:\n        visual_entry['description'] = ai_desc.get('description', '')\n        visual_entry['visual_type'] = ai_desc.get('type', 'unknown')\n        visual_entry['concept'] = ai_desc.get('concept', '')\n    \n    visuals[cluster_id] = visual_entry\n\nprint(f\"âœ… Created visual database with {len(visuals)} unique visuals\")\n\n# Show distribution\nmulti_appearance = sum(1 for v in visuals.values() if v['appearance_count'] > 1)\nprint(f\"   Visuals appearing once: {len(visuals) - multi_appearance}\")\nprint(f\"   Visuals appearing multiple times: {multi_appearance}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:03:30.232363Z","iopub.execute_input":"2026-02-09T07:03:30.232666Z","iopub.status.idle":"2026-02-09T07:03:30.241315Z","shell.execute_reply.started":"2026-02-09T07:03:30.232638Z","shell.execute_reply":"2026-02-09T07:03:30.240628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 10: Create Enriched Transcript\n# ====================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸ“ CREATING ENRICHED TRANSCRIPT\")\nprint(\"=\"*70)\n\n# Build mapping from reference to cluster\nreference_to_visual = {}\n\nfor frame in valid_frames:\n    frame_id = frame['frame_id']\n    reference_id = frame['reference_id']\n    \n    if frame_id in frame_to_cluster:\n        cluster_id = frame_to_cluster[frame_id]\n        \n        # Map reference to visual (use first occurrence)\n        if reference_id not in reference_to_visual:\n            reference_to_visual[reference_id] = cluster_id\n\n# Create transcript entries\ntranscript_entries = []\nreferences = phase2_data.get('references', [])\n\nfor reference in references:\n    reference_id = reference['reference_id']\n    visual_id = reference_to_visual.get(reference_id)\n    \n    entry = {\n        \"timestamp_ms\": reference['timestamp_ms'],\n        \"text\": reference['text'],\n        \"visual_id\": visual_id,\n        \"reference_type\": reference.get('reference_type', 'unknown'),\n        \"detection_method\": reference.get('detection_method', 'unknown')\n    }\n    \n    # Add visual metadata if available\n    if visual_id and visual_id in visuals:\n        visual = visuals[visual_id]\n        entry['visual_description'] = visual.get('description')\n        entry['visual_type'] = visual.get('visual_type')\n        entry['ocr_text'] = visual.get('ocr_text')\n    \n    transcript_entries.append(entry)\n\nprint(f\"âœ… Created {len(transcript_entries)} transcript entries\")\nprint(f\"   Entries with visuals: {sum(1 for e in transcript_entries if e['visual_id'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:03:36.535859Z","iopub.execute_input":"2026-02-09T07:03:36.536221Z","iopub.status.idle":"2026-02-09T07:03:36.545238Z","shell.execute_reply.started":"2026-02-09T07:03:36.536189Z","shell.execute_reply":"2026-02-09T07:03:36.544353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================================\n# CELL 11: Save Final Results\n# ====================================================================\n\n# Create final enriched transcript\nenriched_transcript = {\n    \"video_id\": video_id,\n    \"video_url\": phase1_data.get('video_url', ''),\n    \"video_title\": phase1_data.get('video_title', ''),\n    \"duration_ms\": phase1_data['duration_ms'],\n    \n    # Main outputs\n    \"transcript\": transcript_entries,\n    \"visuals\": visuals,\n    \n    # Metadata\n    \"metadata\": {\n        \"total_utterances\": len(phase1_data['utterances']),\n        \"total_references\": len(references),\n        \"total_frames_extracted\": len(frames),\n        \"valid_frames\": len(valid_frames),\n        \"unique_visuals\": len(visuals),\n        \"clip_model\": CONFIG['clip_model'],\n        \"clustering_eps\": CONFIG['clustering']['eps']\n    }\n}\n\n# Save main output\noutput_file = os.path.join(CONFIG['output_dir'], CONFIG['final_output_file'])\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(enriched_transcript, f, ensure_ascii=False, indent=2)\n\n# Save clustering details (intermediate)\nif CONFIG['save_intermediate']:\n    clustering_file = f\"{CONFIG['output_dir']}/phase4_clustering_{video_id}.json\"\n    clustering_data = {\n        \"clusters\": {k: v for k, v in clusters.items()},\n        \"frame_to_cluster\": frame_to_cluster,\n        \"cluster_representatives\": cluster_representatives,\n        \"cluster_sizes\": {k: len(v) for k, v in clusters.items()}\n    }\n    with open(clustering_file, 'w', encoding='utf-8') as f:\n        json.dump(clustering_data, f, ensure_ascii=False, indent=2)\n    print(f\"ðŸ’¾ Clustering details: {clustering_file}\")\n\n# Save human-readable report\nreport_file = f\"{CONFIG['output_dir']}/phase4_report_{video_id}.txt\"\nwith open(report_file, 'w', encoding='utf-8') as f:\n    f.write(\"ENRICHED TRANSCRIPT REPORT\\n\")\n    f.write(f\"Video ID: {video_id}\\n\")\n    f.write(\"=\"*70 + \"\\n\\n\")\n    \n    f.write(\"VISUAL DATABASE\\n\")\n    f.write(\"-\"*70 + \"\\n\")\n    for visual_id, visual in list(visuals.items())[:20]:  # First 20\n        f.write(f\"\\n{visual_id}:\\n\")\n        f.write(f\"  Appears: {visual['appearance_count']} times\\n\")\n        f.write(f\"  Times: {', '.join([str(t/1000) + 's' for t in visual['appears_at_ms'][:5]])}...\\n\")\n        if visual.get('description'):\n            f.write(f\"  Description: {visual['description'][:100]}...\\n\")\n        if visual.get('ocr_text'):\n            f.write(f\"  OCR: {visual['ocr_text'][:100]}...\\n\")\n    \n    if len(visuals) > 20:\n        f.write(f\"\\n... and {len(visuals) - 20} more visuals\\n\")\n    \n    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n    f.write(\"\\nTRANSCRIPT ENTRIES\\n\")\n    f.write(\"-\"*70 + \"\\n\")\n    for i, entry in enumerate(transcript_entries[:10], 1):  # First 10\n        f.write(f\"\\n[{i}] {entry['timestamp_ms']/1000:.1f}s\\n\")\n        f.write(f\"  Text: {entry['text'][:100]}...\\n\")\n        f.write(f\"  Visual: {entry['visual_id'] or 'None'}\\n\")\n        if entry.get('visual_description'):\n            f.write(f\"  Description: {entry['visual_description'][:80]}...\\n\")\n\nfile_size = os.path.getsize(output_file) / 1024\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… PHASE 4 COMPLETE - PIPELINE FINISHED!\")\nprint(\"=\"*70)\nprint(f\"ðŸ“Š Final Statistics:\")\nprint(f\"   Video ID: {video_id}\")\nprint(f\"   Duration: {enriched_transcript['duration_ms']/1000:.1f}s\")\nprint(f\"   Total references: {enriched_transcript['metadata']['total_references']}\")\nprint(f\"   Unique visuals: {enriched_transcript['metadata']['unique_visuals']}\")\nprint(f\"   Frames analyzed: {enriched_transcript['metadata']['total_frames_extracted']}\")\nprint(f\"\\nðŸ’¾ Output files:\")\nprint(f\"   Main: {output_file} ({file_size:.1f} KB)\")\nprint(f\"   Report: {report_file}\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:03:52.721438Z","iopub.execute_input":"2026-02-09T07:03:52.722026Z","iopub.status.idle":"2026-02-09T07:03:52.737103Z","shell.execute_reply.started":"2026-02-09T07:03:52.721996Z","shell.execute_reply":"2026-02-09T07:03:52.736335Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Integration**","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\n# Define the output directory\nOUTPUT_DIR = \"/kaggle/working/output\"\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\n# 1. File Paths (based on your uploads)\nphase1_file = '/kaggle/working/output/transcript_XNQTWZ87K4I.json' \nenriched_file = '/kaggle/working/output/enriched_transcript.json'\n\n# 2. Load Data\nwith open(phase1_file, 'r', encoding='utf-8') as f:\n    phase1_data = json.load(f)\n\nwith open(enriched_file, 'r', encoding='utf-8') as f:\n    enriched_data = json.load(f)\n\n# The fix: 'visuals' in your file is a dict mapping ID -> Data\nvisual_lookup = enriched_data.get('visuals', {})\n\n# 3. Create a sorted timeline of visual detection points\nvisual_timeline = []\nfor entry in enriched_data.get('transcript', []):\n    if entry.get('visual_id'):\n        visual_timeline.append({\n            'ts': entry['timestamp_ms'],\n            'id': entry['visual_id']\n        })\nvisual_timeline = sorted(visual_timeline, key=lambda x: x['ts'])\n\ndef find_active_visual(start_ms, end_ms):\n    \"\"\"Finds which visual state was active during this speech segment.\"\"\"\n    # Find a visual that was triggered during this specific sentence\n    for event in visual_timeline:\n        if start_ms <= event['ts'] <= end_ms:\n            return event['id']\n    \n    # Fallback: Find the most recent visual state set before this sentence\n    active_id = None\n    for event in visual_timeline:\n        if event['ts'] <= start_ms:\n            active_id = event['id']\n        else:\n            break\n    return active_id\n\n# 4. Perform the Join\nfinal_entries = []\nfor utt in phase1_data.get('utterances', []):\n    v_id = find_active_visual(utt['start_ms'], utt['end_ms'])\n    v_info = visual_lookup.get(v_id)\n    \n    # Clean up the AI description if it's wrapped in markdown code blocks\n    raw_desc = v_info.get('description', '') if v_info else \"\"\n    clean_desc = raw_desc.replace('```json', '').replace('```', '').strip()\n    \n    entry = {\n        \"timestamp_ms\": utt['start_ms'],\n        \"time\": f\"{utt['start_ms']/1000:.1f}s\",\n        \"text\": utt['text'],\n        \"visual_id\": v_id,\n        \"visual_details\": clean_desc,\n        \"elements_on_screen\": v_info.get('visual_elements', []) if v_info else []\n    }\n    final_entries.append(entry)\n\n# 5. Save the final integrated file\nfinal_output = {\n    \"video_info\": {\n        \"title\": phase1_data.get('video_title'),\n        \"url\": phase1_data.get('video_url')\n    },\n    \"multimodal_transcript\": final_entries\n}\n\noutput_path = os.path.join(OUTPUT_DIR, \"FINAL_INTEGRATED_OUTPUT.json\")\nwith open(output_path, 'w', encoding='utf-8') as f:\n    json.dump(final_output, f, ensure_ascii=False, indent=2)\n\nprint(f\"âœ… Integration complete!\")\nprint(f\"ðŸ“‚ Final file saved to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:38:15.868363Z","iopub.execute_input":"2026-02-09T07:38:15.868689Z","iopub.status.idle":"2026-02-09T07:38:15.884323Z","shell.execute_reply.started":"2026-02-09T07:38:15.868660Z","shell.execute_reply":"2026-02-09T07:38:15.883531Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**final transcript**","metadata":{}},{"cell_type":"markdown","source":"**phase1+phase4**","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\n# 1. Setup paths\nINPUT_TRANSCRIPT = '/kaggle/working/output/transcript_XNQTWZ87K4I.json'\nINPUT_ENRICHED = '/kaggle/working/output/enriched_transcript.json'\nOUTPUT_PATH = '/kaggle/working/output/FINAL_NARRATIVE_TRANSCRIPT.txt'\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 2. Load the data from your phases\nwith open(INPUT_TRANSCRIPT, 'r', encoding='utf-8') as f:\n    phase1 = json.load(f)\nwith open(INPUT_ENRICHED, 'r', encoding='utf-8') as f:\n    enriched = json.load(f)\n\nvisual_lookup = enriched.get('visuals', {})\nvisual_events = sorted(\n    [e for e in enriched.get('transcript', []) if e.get('visual_id')],\n    key=lambda x: x['timestamp_ms']\n)\n\n# 3. Process and Build the Text Transcript\nlines = []\nlines.append(f\"NARRATIVE TRANSCRIPT: {phase1.get('video_title', 'GDPR Video')}\")\nlines.append(\"=\"*80 + \"\\n\")\n\nlast_visual_id = None\n\nfor utt in phase1.get('utterances', []):\n    start_ms = utt['start_ms']\n    # Format time as [MM:SS]\n    time_str = f\"[{int(start_ms//60000):02d}:{int((start_ms%60000)//1000):02d}]\"\n    \n    # Check for visual context update\n    current_visual_id = None\n    for event in visual_events:\n        if event['timestamp_ms'] <= start_ms:\n            current_visual_id = event['visual_id']\n        else:\n            break\n    \n    # If the visual state changed, insert a descriptive block\n    if current_visual_id and current_visual_id != last_visual_id:\n        v_info = visual_lookup.get(current_visual_id, {})\n        raw_desc = v_info.get('description', '')\n        \n        # Clean the AI description (removes JSON code blocks)\n        try:\n            clean_json_str = raw_desc.replace('```json', '').replace('```', '').strip()\n            desc_obj = json.loads(clean_json_str)\n            main_desc = desc_obj.get('description', '')\n            elements = desc_obj.get('visual_elements', [])\n        except:\n            main_desc = raw_desc\n            elements = []\n\n        lines.append(f\"\\n--- [VISUAL CONTEXT CHANGE] ---\")\n        lines.append(f\"SCENE: {main_desc}\")\n        if elements:\n            lines.append(f\"VISIBLE ON SCREEN: {', '.join(elements)}\")\n        lines.append(\"-\" * 31 + \"\\n\")\n        \n        last_visual_id = current_visual_id\n\n    # Append the spoken text\n    lines.append(f\"{time_str} {utt['text']}\")\n\n# 4. Write to the final text file\nwith open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n    f.write(\"\\n\".join(lines))\n\nprint(f\"âœ… Readable transcript successfully created at: {OUTPUT_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T07:44:42.282978Z","iopub.execute_input":"2026-02-09T07:44:42.283327Z","iopub.status.idle":"2026-02-09T07:44:42.295890Z","shell.execute_reply.started":"2026-02-09T07:44:42.283298Z","shell.execute_reply":"2026-02-09T07:44:42.295229Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**phase1+phase3**","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\n# 1. Setup paths\nINPUT_TRANSCRIPT = '/kaggle/working/output/transcript_XNQTWZ87K4I.json'\nINPUT_PHASE3 = '/kaggle/working/output/phase3_frames_XNQTWZ87K4I.json'\nOUTPUT_PATH = '/kaggle/working/output/PHASE3_NARRATIVE_TRANSCRIPT.txt'\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 2. Load the data\nwith open(INPUT_TRANSCRIPT, 'r', encoding='utf-8') as f:\n    phase1 = json.load(f)\nwith open(INPUT_PHASE3, 'r', encoding='utf-8') as f:\n    phase3 = json.load(f)\n\n# Sort frames by timestamp to ensure chronological order\nframes = sorted(phase3.get('frames', []), key=lambda x: x['timestamp_ms'])\n\n# 3. Build the Narrative Text\nlines = []\nlines.append(f\"PHASE 3 INTEGRATED TRANSCRIPT: {phase1.get('video_title', 'GDPR Video')}\")\nlines.append(\"=\"*80 + \"\\n\")\n\nframe_idx = 0\nnum_frames = len(frames)\n\nfor utt in phase1.get('utterances', []):\n    start_ms = utt['start_ms']\n    end_ms = utt['end_ms']\n    time_str = f\"[{int(start_ms//60000):02d}:{int((start_ms%60000)//1000):02d}]\"\n    \n    # Insert visual signals that occur before or during this speech segment\n    while frame_idx < num_frames and frames[frame_idx]['timestamp_ms'] <= end_ms:\n        frame = frames[frame_idx]\n        raw_ai = frame.get('ai_description', {})\n        raw_desc_str = raw_ai.get('description', '')\n        \n        # Parse the Vision AI's JSON description\n        try:\n            clean_json_str = raw_desc_str.replace('```json', '').replace('```', '').strip()\n            desc_obj = json.loads(clean_json_str)\n            main_desc = desc_obj.get('description', 'No description available.')\n            elements = desc_obj.get('visual_elements', [])\n            v_type = desc_obj.get('type', 'Unknown')\n        except:\n            main_desc = raw_desc_str\n            elements = []\n            v_type = \"Unknown\"\n\n        lines.append(f\"\\n[ VISUAL SIGNAL @ {frame['timestamp_ms']/1000:.1f}s | ID: {frame['frame_id']} ]\")\n        lines.append(f\"TYPE: {v_type}\")\n        lines.append(f\"SCENE: {main_desc}\")\n        if elements:\n            lines.append(f\"UI ELEMENTS: {', '.join(elements)}\")\n        lines.append(\"-\" * 50 + \"\\n\")\n        \n        frame_idx += 1\n\n    # Append the spoken text\n    lines.append(f\"{time_str} {utt['text']}\")\n\n# 4. Save to file\nwith open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n    f.write(\"\\n\".join(lines))\n\nprint(f\"âœ… Phase 3 integrated transcript successfully created at: {OUTPUT_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:55:19.014966Z","iopub.execute_input":"2026-02-09T08:55:19.015839Z","iopub.status.idle":"2026-02-09T08:55:19.029511Z","shell.execute_reply.started":"2026-02-09T08:55:19.015799Z","shell.execute_reply":"2026-02-09T08:55:19.028710Z"}},"outputs":[],"execution_count":null}]}