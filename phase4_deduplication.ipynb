{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Phase 4: Visual Deduplication & Enriched Transcript\n",
    "## Final step: Cluster similar frames and link speech to visuals\n",
    "\n",
    "This notebook:\n",
    "- Loads Phase 1, 2, 3 outputs\n",
    "- Uses CLIP embeddings to find visually similar frames\n",
    "- Clusters duplicates using DBSCAN\n",
    "- Selects best representative per cluster\n",
    "- Creates final enriched transcript linking speech ‚Üí visuals\n",
    "\n",
    "**Output:** JSON with timestamped transcript + visual database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 1: Install Dependencies\n",
    "# ====================================================================\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# Core dependencies\n",
    "!pip install -q transformers torch pillow scikit-learn numpy\n",
    "\n",
    "# CLIP model\n",
    "!pip install -q sentence-transformers  # Includes CLIP\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 2: Import Libraries\n",
    "# ====================================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gc\n",
    "\n",
    "print(f\"üñ•Ô∏è  Device: {'CUDA (GPU)' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 3: Configuration\n",
    "# ====================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Input files from previous phases\n",
    "    \"phase1_file\": \"/kaggle/working/output/transcript_XXXXX.json\",  # ‚ö†Ô∏è UPDATE\n",
    "    \"phase2_file\": \"/kaggle/working/output/phase2_references_XXXXX.json\",  # ‚ö†Ô∏è UPDATE\n",
    "    \"phase3_file\": \"/kaggle/working/output/phase3_frames_XXXXX.json\",  # ‚ö†Ô∏è UPDATE\n",
    "    \n",
    "    # CLIP model for visual similarity\n",
    "    \"clip_model\": \"clip-ViT-B-32\",  # Options: clip-ViT-B-32, clip-ViT-L-14 (larger, better)\n",
    "    \n",
    "    # Clustering parameters (DBSCAN)\n",
    "    \"clustering\": {\n",
    "        \"eps\": 0.3,  # Distance threshold (0.2-0.4, lower=stricter)\n",
    "        \"min_samples\": 1,  # Minimum frames to form cluster\n",
    "        \"metric\": \"cosine\"  # Cosine distance for CLIP embeddings\n",
    "    },\n",
    "    \n",
    "    # Representative selection\n",
    "    \"select_best_by\": \"quality\",  # \"quality\" or \"central\" (most similar to cluster center)\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"/kaggle/working/output\",\n",
    "    \"final_output_file\": \"enriched_transcript.json\",\n",
    "    \"save_intermediate\": True\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   CLIP model: {CONFIG['clip_model']}\")\n",
    "print(f\"   DBSCAN eps: {CONFIG['clustering']['eps']}\")\n",
    "print(f\"   Min samples: {CONFIG['clustering']['min_samples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 4: Load All Previous Phase Data\n",
    "# ====================================================================\n",
    "\n",
    "print(\"üìÇ Loading Phase 1, 2, 3 outputs...\\n\")\n",
    "\n",
    "# Load Phase 1 (transcript)\n",
    "with open(CONFIG['phase1_file'], 'r', encoding='utf-8') as f:\n",
    "    phase1_data = json.load(f)\n",
    "print(f\"‚úÖ Phase 1: {len(phase1_data['utterances'])} utterances\")\n",
    "\n",
    "# Load Phase 2 (references)\n",
    "with open(CONFIG['phase2_file'], 'r', encoding='utf-8') as f:\n",
    "    phase2_data = json.load(f)\n",
    "print(f\"‚úÖ Phase 2: {len(phase2_data['references'])} references\")\n",
    "\n",
    "# Load Phase 3 (frames)\n",
    "with open(CONFIG['phase3_file'], 'r', encoding='utf-8') as f:\n",
    "    phase3_data = json.load(f)\n",
    "print(f\"‚úÖ Phase 3: {len(phase3_data['frames'])} frames\")\n",
    "\n",
    "video_id = phase1_data['video_id']\n",
    "frames = phase3_data['frames']\n",
    "\n",
    "# Filter valid frames (with existing paths)\n",
    "valid_frames = [\n",
    "    f for f in frames \n",
    "    if f.get('frame_path') and os.path.exists(f['frame_path'])\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Video ID: {video_id}\")\n",
    "print(f\"   Total frames: {len(frames)}\")\n",
    "print(f\"   Valid frames: {len(valid_frames)}\")\n",
    "\n",
    "if len(valid_frames) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No valid frames found! Check Phase 3 output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clip_embedder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 5: CLIP Embedding Functions\n",
    "# ====================================================================\n",
    "\n",
    "class CLIPEmbedder:\n",
    "    \"\"\"Generate CLIP embeddings for images.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"clip-ViT-B-32\"):\n",
    "        print(f\"ü§ñ Loading CLIP model: {model_name}...\")\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.device = device\n",
    "        print(f\"‚úÖ CLIP model loaded on {device}\")\n",
    "    \n",
    "    def embed_image(self, image_path: str) -> np.ndarray:\n",
    "        \"\"\"Generate embedding for single image.\"\"\"\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            embedding = self.model.encode(img, convert_to_numpy=True)\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error encoding {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def embed_images_batch(self, image_paths: List[str], batch_size: int = 8) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for multiple images in batches.\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        print(f\"   Encoding {len(image_paths)} images in batches of {batch_size}...\")\n",
    "        \n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            \n",
    "            # Load images\n",
    "            images = []\n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    img = Image.open(path).convert('RGB')\n",
    "                    images.append(img)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Skipping {path}: {e}\")\n",
    "                    images.append(None)\n",
    "            \n",
    "            # Encode batch\n",
    "            valid_images = [img for img in images if img is not None]\n",
    "            if valid_images:\n",
    "                batch_embeddings = self.model.encode(\n",
    "                    valid_images,\n",
    "                    convert_to_numpy=True,\n",
    "                    show_progress_bar=False\n",
    "                )\n",
    "                embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"   Progress: {i+len(batch_paths)}/{len(image_paths)}\")\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "\n",
    "print(\"‚úÖ CLIP embedder class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clustering_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 6: Clustering & Selection Functions\n",
    "# ====================================================================\n",
    "\n",
    "def cluster_embeddings(embeddings: np.ndarray, eps: float = 0.3, \n",
    "                      min_samples: int = 1, metric: str = 'cosine') -> np.ndarray:\n",
    "    \"\"\"Cluster embeddings using DBSCAN.\"\"\"\n",
    "    print(f\"\\nüîç Clustering {len(embeddings)} embeddings...\")\n",
    "    print(f\"   eps={eps}, min_samples={min_samples}, metric={metric}\")\n",
    "    \n",
    "    # DBSCAN clustering\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n",
    "    labels = clustering.fit_predict(embeddings)\n",
    "    \n",
    "    # Count clusters\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    "    print(f\"‚úÖ Found {n_clusters} clusters + {n_noise} noise points\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def select_best_representative(frames: List[Dict], frame_indices: List[int],\n",
    "                              embeddings: np.ndarray, method: str = \"quality\") -> int:\n",
    "    \"\"\"Select best representative frame from cluster.\"\"\"\n",
    "    if len(frame_indices) == 1:\n",
    "        return frame_indices[0]\n",
    "    \n",
    "    if method == \"quality\":\n",
    "        # Select frame with highest quality score\n",
    "        qualities = [\n",
    "            frames[idx].get('quality', {}).get('quality_score', 0)\n",
    "            for idx in frame_indices\n",
    "        ]\n",
    "        best_local_idx = np.argmax(qualities)\n",
    "        return frame_indices[best_local_idx]\n",
    "    \n",
    "    elif method == \"central\":\n",
    "        # Select frame closest to cluster centroid\n",
    "        cluster_embeddings = embeddings[frame_indices]\n",
    "        centroid = cluster_embeddings.mean(axis=0)\n",
    "        \n",
    "        # Find closest to centroid\n",
    "        similarities = cosine_similarity([centroid], cluster_embeddings)[0]\n",
    "        best_local_idx = np.argmax(similarities)\n",
    "        return frame_indices[best_local_idx]\n",
    "    \n",
    "    return frame_indices[0]\n",
    "\n",
    "print(\"‚úÖ Clustering functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 7: Generate CLIP Embeddings\n",
    "# ====================================================================\n",
    "\n",
    "if len(valid_frames) > 0:\n",
    "    print(\"=\"*70)\n",
    "    print(\"üöÄ GENERATING CLIP EMBEDDINGS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize embedder\n",
    "    embedder = CLIPEmbedder(CONFIG['clip_model'])\n",
    "    \n",
    "    # Extract frame paths\n",
    "    frame_paths = [f['frame_path'] for f in valid_frames]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = embedder.embed_images_batch(frame_paths, batch_size=8)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated {len(embeddings)} embeddings\")\n",
    "    print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del embedder\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No valid frames to embed\")\n",
    "    embeddings = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cluster_frames",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 8: Cluster Similar Frames\n",
    "# ====================================================================\n",
    "\n",
    "if len(embeddings) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç CLUSTERING VISUAL FRAMES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Cluster embeddings\n",
    "    labels = cluster_embeddings(\n",
    "        embeddings,\n",
    "        eps=CONFIG['clustering']['eps'],\n",
    "        min_samples=CONFIG['clustering']['min_samples'],\n",
    "        metric=CONFIG['clustering']['metric']\n",
    "    )\n",
    "    \n",
    "    # Organize clusters\n",
    "    clusters = defaultdict(list)\n",
    "    frame_to_cluster = {}\n",
    "    \n",
    "    for i, (frame, label) in enumerate(zip(valid_frames, labels)):\n",
    "        frame_id = frame['frame_id']\n",
    "        \n",
    "        if label == -1:\n",
    "            # Noise point - unique visual\n",
    "            cluster_id = f\"VISUAL_{i:03d}_SINGLE\"\n",
    "        else:\n",
    "            # Part of cluster\n",
    "            cluster_id = f\"VISUAL_{label:03d}\"\n",
    "        \n",
    "        clusters[cluster_id].append(i)\n",
    "        frame_to_cluster[frame_id] = cluster_id\n",
    "    \n",
    "    print(f\"\\nüìä Clustering results:\")\n",
    "    print(f\"   Total clusters: {len(clusters)}\")\n",
    "    print(f\"   Average cluster size: {np.mean([len(v) for v in clusters.values()]):.1f}\")\n",
    "    print(f\"   Largest cluster: {max(len(v) for v in clusters.values())} frames\")\n",
    "    \n",
    "    # Select representatives\n",
    "    print(f\"\\nüéØ Selecting best representatives (method: {CONFIG['select_best_by']})...\")\n",
    "    cluster_representatives = {}\n",
    "    \n",
    "    for cluster_id, frame_indices in clusters.items():\n",
    "        best_idx = select_best_representative(\n",
    "            valid_frames,\n",
    "            frame_indices,\n",
    "            embeddings,\n",
    "            method=CONFIG['select_best_by']\n",
    "        )\n",
    "        cluster_representatives[cluster_id] = best_idx\n",
    "    \n",
    "    print(f\"‚úÖ Selected {len(cluster_representatives)} representatives\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No embeddings to cluster\")\n",
    "    clusters = {}\n",
    "    frame_to_cluster = {}\n",
    "    cluster_representatives = {}\n",
    "    labels = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_visual_database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 9: Create Visual Database\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üé® CREATING VISUAL DATABASE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "visuals = {}\n",
    "\n",
    "for cluster_id, frame_indices in clusters.items():\n",
    "    if not frame_indices:\n",
    "        continue\n",
    "    \n",
    "    # Get representative frame\n",
    "    rep_idx = cluster_representatives.get(cluster_id, frame_indices[0])\n",
    "    rep_frame = valid_frames[rep_idx]\n",
    "    \n",
    "    # Collect all timestamps where this visual appears\n",
    "    timestamps = [valid_frames[idx]['timestamp_ms'] for idx in frame_indices]\n",
    "    \n",
    "    # Build visual entry\n",
    "    visual_entry = {\n",
    "        \"representative_frame\": rep_frame['frame_path'],\n",
    "        \"thumbnail\": rep_frame.get('thumbnail_path'),\n",
    "        \"appears_at_ms\": sorted(timestamps),\n",
    "        \"appearance_count\": len(timestamps),\n",
    "        \"quality\": rep_frame.get('quality', {}),\n",
    "        \"frame_ids\": [valid_frames[idx]['frame_id'] for idx in frame_indices]\n",
    "    }\n",
    "    \n",
    "    # Add OCR data if available\n",
    "    ocr_data = rep_frame.get('ocr_data')\n",
    "    if ocr_data and ocr_data.get('text'):\n",
    "        visual_entry['ocr_text'] = ocr_data['text']\n",
    "        visual_entry['ocr_confidence'] = ocr_data.get('confidence', 0)\n",
    "    \n",
    "    # Add AI description if available\n",
    "    ai_desc = rep_frame.get('ai_description')\n",
    "    if ai_desc:\n",
    "        visual_entry['description'] = ai_desc.get('description', '')\n",
    "        visual_entry['visual_type'] = ai_desc.get('type', 'unknown')\n",
    "        visual_entry['concept'] = ai_desc.get('concept', '')\n",
    "    \n",
    "    visuals[cluster_id] = visual_entry\n",
    "\n",
    "print(f\"‚úÖ Created visual database with {len(visuals)} unique visuals\")\n",
    "\n",
    "# Show distribution\n",
    "multi_appearance = sum(1 for v in visuals.values() if v['appearance_count'] > 1)\n",
    "print(f\"   Visuals appearing once: {len(visuals) - multi_appearance}\")\n",
    "print(f\"   Visuals appearing multiple times: {multi_appearance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_enriched_transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 10: Create Enriched Transcript\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù CREATING ENRICHED TRANSCRIPT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Build mapping from reference to cluster\n",
    "reference_to_visual = {}\n",
    "\n",
    "for frame in valid_frames:\n",
    "    frame_id = frame['frame_id']\n",
    "    reference_id = frame['reference_id']\n",
    "    \n",
    "    if frame_id in frame_to_cluster:\n",
    "        cluster_id = frame_to_cluster[frame_id]\n",
    "        \n",
    "        # Map reference to visual (use first occurrence)\n",
    "        if reference_id not in reference_to_visual:\n",
    "            reference_to_visual[reference_id] = cluster_id\n",
    "\n",
    "# Create transcript entries\n",
    "transcript_entries = []\n",
    "references = phase2_data.get('references', [])\n",
    "\n",
    "for reference in references:\n",
    "    reference_id = reference['reference_id']\n",
    "    visual_id = reference_to_visual.get(reference_id)\n",
    "    \n",
    "    entry = {\n",
    "        \"timestamp_ms\": reference['timestamp_ms'],\n",
    "        \"text\": reference['text'],\n",
    "        \"visual_id\": visual_id,\n",
    "        \"reference_type\": reference.get('reference_type', 'unknown'),\n",
    "        \"detection_method\": reference.get('detection_method', 'unknown')\n",
    "    }\n",
    "    \n",
    "    # Add visual metadata if available\n",
    "    if visual_id and visual_id in visuals:\n",
    "        visual = visuals[visual_id]\n",
    "        entry['visual_description'] = visual.get('description')\n",
    "        entry['visual_type'] = visual.get('visual_type')\n",
    "        entry['ocr_text'] = visual.get('ocr_text')\n",
    "    \n",
    "    transcript_entries.append(entry)\n",
    "\n",
    "print(f\"‚úÖ Created {len(transcript_entries)} transcript entries\")\n",
    "print(f\"   Entries with visuals: {sum(1 for e in transcript_entries if e['visual_id'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 11: Save Final Results\n",
    "# ====================================================================\n",
    "\n",
    "# Create final enriched transcript\n",
    "enriched_transcript = {\n",
    "    \"video_id\": video_id,\n",
    "    \"video_url\": phase1_data.get('video_url', ''),\n",
    "    \"video_title\": phase1_data.get('video_title', ''),\n",
    "    \"duration_ms\": phase1_data['duration_ms'],\n",
    "    \n",
    "    # Main outputs\n",
    "    \"transcript\": transcript_entries,\n",
    "    \"visuals\": visuals,\n",
    "    \n",
    "    # Metadata\n",
    "    \"metadata\": {\n",
    "        \"total_utterances\": len(phase1_data['utterances']),\n",
    "        \"total_references\": len(references),\n",
    "        \"total_frames_extracted\": len(frames),\n",
    "        \"valid_frames\": len(valid_frames),\n",
    "        \"unique_visuals\": len(visuals),\n",
    "        \"clip_model\": CONFIG['clip_model'],\n",
    "        \"clustering_eps\": CONFIG['clustering']['eps']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save main output\n",
    "output_file = os.path.join(CONFIG['output_dir'], CONFIG['final_output_file'])\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(enriched_transcript, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save clustering details (intermediate)\n",
    "if CONFIG['save_intermediate']:\n",
    "    clustering_file = f\"{CONFIG['output_dir']}/phase4_clustering_{video_id}.json\"\n",
    "    clustering_data = {\n",
    "        \"clusters\": {k: v for k, v in clusters.items()},\n",
    "        \"frame_to_cluster\": frame_to_cluster,\n",
    "        \"cluster_representatives\": cluster_representatives,\n",
    "        \"cluster_sizes\": {k: len(v) for k, v in clusters.items()}\n",
    "    }\n",
    "    with open(clustering_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(clustering_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"üíæ Clustering details: {clustering_file}\")\n",
    "\n",
    "# Save human-readable report\n",
    "report_file = f\"{CONFIG['output_dir']}/phase4_report_{video_id}.txt\"\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"ENRICHED TRANSCRIPT REPORT\\n\")\n",
    "    f.write(f\"Video ID: {video_id}\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"VISUAL DATABASE\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    for visual_id, visual in list(visuals.items())[:20]:  # First 20\n",
    "        f.write(f\"\\n{visual_id}:\\n\")\n",
    "        f.write(f\"  Appears: {visual['appearance_count']} times\\n\")\n",
    "        f.write(f\"  Times: {', '.join([str(t/1000) + 's' for t in visual['appears_at_ms'][:5]])}...\\n\")\n",
    "        if visual.get('description'):\n",
    "            f.write(f\"  Description: {visual['description'][:100]}...\\n\")\n",
    "        if visual.get('ocr_text'):\n",
    "            f.write(f\"  OCR: {visual['ocr_text'][:100]}...\\n\")\n",
    "    \n",
    "    if len(visuals) > 20:\n",
    "        f.write(f\"\\n... and {len(visuals) - 20} more visuals\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    f.write(\"\\nTRANSCRIPT ENTRIES\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    for i, entry in enumerate(transcript_entries[:10], 1):  # First 10\n",
    "        f.write(f\"\\n[{i}] {entry['timestamp_ms']/1000:.1f}s\\n\")\n",
    "        f.write(f\"  Text: {entry['text'][:100]}...\\n\")\n",
    "        f.write(f\"  Visual: {entry['visual_id'] or 'None'}\\n\")\n",
    "        if entry.get('visual_description'):\n",
    "            f.write(f\"  Description: {entry['visual_description'][:80]}...\\n\")\n",
    "\n",
    "file_size = os.path.getsize(output_file) / 1024\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PHASE 4 COMPLETE - PIPELINE FINISHED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Final Statistics:\")\n",
    "print(f\"   Video ID: {video_id}\")\n",
    "print(f\"   Duration: {enriched_transcript['duration_ms']/1000:.1f}s\")\n",
    "print(f\"   Total references: {enriched_transcript['metadata']['total_references']}\")\n",
    "print(f\"   Unique visuals: {enriched_transcript['metadata']['unique_visuals']}\")\n",
    "print(f\"   Frames analyzed: {enriched_transcript['metadata']['total_frames_extracted']}\")\n",
    "print(f\"\\nüíæ Output files:\")\n",
    "print(f\"   Main: {output_file} ({file_size:.1f} KB)\")\n",
    "print(f\"   Report: {report_file}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 12: Display Sample Results\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\nüìã Sample Transcript Entries (first 5):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, entry in enumerate(transcript_entries[:5], 1):\n",
    "    print(f\"\\n[{i}] {entry['timestamp_ms']/1000:.1f}s\")\n",
    "    print(f\"  Text: {entry['text'][:100]}...\")\n",
    "    print(f\"  Visual ID: {entry['visual_id'] or 'None'}\")\n",
    "    \n",
    "    if entry.get('visual_description'):\n",
    "        print(f\"  Description: {entry['visual_description'][:80]}...\")\n",
    "    \n",
    "    if entry.get('ocr_text'):\n",
    "        print(f\"  OCR: {entry['ocr_text'][:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüé® Top Recurring Visuals:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sort by appearance count\n",
    "sorted_visuals = sorted(\n",
    "    visuals.items(),\n",
    "    key=lambda x: x[1]['appearance_count'],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for i, (visual_id, visual) in enumerate(sorted_visuals[:5], 1):\n",
    "    print(f\"\\n[{i}] {visual_id}\")\n",
    "    print(f\"  Appears: {visual['appearance_count']} times\")\n",
    "    print(f\"  First at: {visual['appears_at_ms'][0]/1000:.1f}s\")\n",
    "    \n",
    "    if visual.get('description'):\n",
    "        print(f\"  Description: {visual['description'][:80]}...\")\n",
    "    \n",
    "    if visual.get('visual_type'):\n",
    "        print(f\"  Type: {visual['visual_type']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚úÖ All done! Your enriched transcript is ready.\")\n",
    "print(f\"üìÅ Download from: {CONFIG['output_dir']}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_clusters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 13: Visualize Clustering (Optional)\n",
    "# ====================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "if len(embeddings) > 1:\n",
    "    print(\"\\nüìä Generating cluster visualization...\")\n",
    "    \n",
    "    # Reduce dimensions for visualization\n",
    "    if len(embeddings) > 50:\n",
    "        perplexity = min(30, len(embeddings) - 1)\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    else:\n",
    "        # Use PCA for small datasets\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Color by cluster\n",
    "    unique_labels = set(labels)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for label, color in zip(unique_labels, colors):\n",
    "        if label == -1:\n",
    "            # Noise points in black\n",
    "            color = 'black'\n",
    "            marker = 'x'\n",
    "            label_name = 'Noise'\n",
    "        else:\n",
    "            marker = 'o'\n",
    "            label_name = f'Cluster {label}'\n",
    "        \n",
    "        mask = labels == label\n",
    "        ax.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            c=[color],\n",
    "            marker=marker,\n",
    "            s=100,\n",
    "            alpha=0.6,\n",
    "            label=label_name if label in [-1, 0, 1, 2] else None,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "    \n",
    "    # Mark representatives\n",
    "    rep_indices = list(cluster_representatives.values())\n",
    "    ax.scatter(\n",
    "        embeddings_2d[rep_indices, 0],\n",
    "        embeddings_2d[rep_indices, 1],\n",
    "        c='none',\n",
    "        marker='o',\n",
    "        s=300,\n",
    "        edgecolors='red',\n",
    "        linewidth=2,\n",
    "        label='Representatives'\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Visual Frame Clustering (CLIP + DBSCAN)\\nRed circles = cluster representatives',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Dimension 1', fontsize=12)\n",
    "    ax.set_ylabel('Dimension 2', fontsize=12)\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/phase4_clusters_{video_id}.png\", \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Cluster visualization saved!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Not enough data for visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
