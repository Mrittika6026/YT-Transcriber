{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# YouTube Video Transcription Pipeline\n",
    "## Optimized for Kaggle - No OOM Errors\n",
    "\n",
    "This notebook:\n",
    "- Downloads YouTube videos\n",
    "- Extracts audio\n",
    "- Transcribes using OpenAI Whisper (memory-efficient)\n",
    "- Creates timestamped utterances\n",
    "- Exports JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 1: Install Dependencies\n",
    "# ====================================================================\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install -q yt-dlp openai-whisper\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 2: Import Libraries\n",
    "# ====================================================================\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "print(f\"üñ•Ô∏è  Device: {'CUDA (GPU)' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    # Clear any existing cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 3: Configuration\n",
    "# ====================================================================\n",
    "# ‚ö†Ô∏è CHANGE THIS to your YouTube video URL\n",
    "VIDEO_URL = \"https://youtu.be/dQw4w9WgXcQ\"  # Replace with your video\n",
    "\n",
    "# Model size options (smaller = less memory):\n",
    "# \"tiny\"   - ~1GB VRAM, fastest, least accurate\n",
    "# \"base\"   - ~1GB VRAM, fast, decent accuracy  ‚úÖ RECOMMENDED for Kaggle\n",
    "# \"small\"  - ~2GB VRAM, good accuracy\n",
    "# \"medium\" - ~5GB VRAM, better accuracy\n",
    "# \"large\"  - ~10GB VRAM, best accuracy (may OOM on Kaggle)\n",
    "\n",
    "CONFIG = {\n",
    "    \"model_size\": \"base\",  # Change to \"small\" or \"medium\" if you have enough memory\n",
    "    \"language\": \"en\",      # \"en\" for English, \"es\" for Spanish, None for auto-detect\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"output_dir\": \"/kaggle/working/output\"\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"/kaggle/working/videos\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/audio\", exist_ok=True)\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Model: {CONFIG['model_size']}\")\n",
    "print(f\"   Language: {CONFIG['language'] or 'auto-detect'}\")\n",
    "print(f\"   Device: {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 4: Download Video\n",
    "# ====================================================================\n",
    "def download_youtube_video(url: str):\n",
    "    \"\"\"Download YouTube video using yt-dlp.\"\"\"\n",
    "    print(f\"üì• Downloading video from: {url}\")\n",
    "    \n",
    "    import yt_dlp\n",
    "    \n",
    "    output_dir = \"/kaggle/working/videos\"\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'format': 'best[ext=mp4]/best',\n",
    "        'outtmpl': os.path.join(output_dir, '%(id)s.%(ext)s'),\n",
    "        'quiet': True,\n",
    "        'no_warnings': True,\n",
    "    }\n",
    "    \n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=True)\n",
    "        video_id = info['id']\n",
    "        video_title = info.get('title', 'Unknown')\n",
    "        duration = info.get('duration', 0)\n",
    "        video_path = os.path.join(output_dir, f\"{video_id}.mp4\")\n",
    "    \n",
    "    print(f\"‚úÖ Video downloaded: {video_title}\")\n",
    "    print(f\"   Duration: {duration//60}m {duration%60}s\")\n",
    "    return video_path, video_id, video_title, duration\n",
    "\n",
    "# Download the video\n",
    "video_path, video_id, video_title, video_duration = download_youtube_video(VIDEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract_audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 5: Extract Audio\n",
    "# ====================================================================\n",
    "def extract_audio(video_path: str) -> str:\n",
    "    \"\"\"Extract audio from video using ffmpeg.\"\"\"\n",
    "    print(f\"üéµ Extracting audio from: {Path(video_path).name}\")\n",
    "    \n",
    "    video_name = Path(video_path).stem\n",
    "    audio_path = f\"/kaggle/working/audio/{video_name}.mp3\"\n",
    "    \n",
    "    # Extract as MP3 (Whisper handles this well)\n",
    "    cmd = [\n",
    "        'ffmpeg', '-i', video_path,\n",
    "        '-vn', '-acodec', 'libmp3lame',\n",
    "        '-ar', '16000', '-ac', '1',\n",
    "        '-b:a', '64k',\n",
    "        '-y', audio_path\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise Exception(f\"FFmpeg error: {result.stderr}\")\n",
    "    \n",
    "    file_size = os.path.getsize(audio_path) / (1024*1024)\n",
    "    print(f\"‚úÖ Audio extracted: {file_size:.2f} MB\")\n",
    "    return audio_path\n",
    "\n",
    "# Extract audio\n",
    "audio_path = extract_audio(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transcribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 6: Transcribe Audio with Whisper\n",
    "# ====================================================================\n",
    "def transcribe_with_whisper(audio_path: str, model_size: str, language: str = None):\n",
    "    \"\"\"Transcribe audio using Whisper (memory-efficient).\"\"\"\n",
    "    print(f\"üé§ Loading Whisper model: {model_size}\")\n",
    "    \n",
    "    # Load model with FP16 if on GPU (saves memory)\n",
    "    model = whisper.load_model(\n",
    "        model_size,\n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Transcribing audio...\")\n",
    "    print(\"   This may take a few minutes depending on video length\")\n",
    "    \n",
    "    # Transcribe with options\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        language=language,\n",
    "        fp16=torch.cuda.is_available(),  # Use FP16 on GPU (faster, less memory)\n",
    "        verbose=False,\n",
    "        word_timestamps=True  # Get word-level timestamps\n",
    "    )\n",
    "    \n",
    "    # Clean up model to free memory\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"‚úÖ Transcription complete!\")\n",
    "    print(f\"   Detected language: {result['language']}\")\n",
    "    print(f\"   Segments: {len(result['segments'])}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Transcribe\n",
    "transcription = transcribe_with_whisper(\n",
    "    audio_path,\n",
    "    CONFIG['model_size'],\n",
    "    CONFIG['language']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 7: Process Results\n",
    "# ====================================================================\n",
    "def create_utterances(transcription_result):\n",
    "    \"\"\"Convert Whisper segments to utterance format.\"\"\"\n",
    "    utterances = []\n",
    "    \n",
    "    for segment in transcription_result['segments']:\n",
    "        utterances.append({\n",
    "            \"text\": segment['text'].strip(),\n",
    "            \"start_ms\": int(segment['start'] * 1000),\n",
    "            \"end_ms\": int(segment['end'] * 1000),\n",
    "            \"confidence\": segment.get('confidence', 0.0),\n",
    "            \"speaker\": \"default\",\n",
    "            \"words\": segment.get('words', [])  # Word-level timestamps if available\n",
    "        })\n",
    "    \n",
    "    return utterances\n",
    "\n",
    "# Create utterances\n",
    "utterances = create_utterances(transcription)\n",
    "full_transcript = transcription['text']\n",
    "\n",
    "print(f\"‚úÖ Created {len(utterances)} utterances\")\n",
    "print(f\"\\nüìù Transcript preview (first 500 chars):\")\n",
    "print(\"=\"*70)\n",
    "print(full_transcript[:500] + \"...\" if len(full_transcript) > 500 else full_transcript)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 8: Save Results\n",
    "# ====================================================================\n",
    "# Prepare final output\n",
    "result = {\n",
    "    \"video_id\": video_id,\n",
    "    \"video_title\": video_title,\n",
    "    \"video_url\": VIDEO_URL,\n",
    "    \"video_path\": video_path,\n",
    "    \"audio_path\": audio_path,\n",
    "    \"duration_ms\": video_duration * 1000,\n",
    "    \"full_transcript\": full_transcript,\n",
    "    \"utterances\": utterances,\n",
    "    \"utterance_count\": len(utterances),\n",
    "    \"model_used\": f\"whisper-{CONFIG['model_size']}\",\n",
    "    \"language\": transcription['language']\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_file = f\"{CONFIG['output_dir']}/transcript_{video_id}.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Also save just the text\n",
    "text_file = f\"{CONFIG['output_dir']}/transcript_{video_id}.txt\"\n",
    "with open(text_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(full_transcript)\n",
    "\n",
    "file_size = os.path.getsize(output_file) / (1024*1024)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRANSCRIPTION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Results:\")\n",
    "print(f\"   - Video: {video_title}\")\n",
    "print(f\"   - Duration: {video_duration//60}m {video_duration%60}s\")\n",
    "print(f\"   - Language: {transcription['language']}\")\n",
    "print(f\"   - Utterances: {len(utterances)}\")\n",
    "print(f\"   - Transcript length: {len(full_transcript)} characters\")\n",
    "print(f\"\\nüíæ Files saved:\")\n",
    "print(f\"   - JSON: {output_file} ({file_size:.2f} MB)\")\n",
    "print(f\"   - Text: {text_file}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 9: Display Sample Results\n",
    "# ====================================================================\n",
    "print(\"\\nüìÑ First 5 utterances:\")\n",
    "print(\"=\"*70)\n",
    "for i, utt in enumerate(utterances[:5], 1):\n",
    "    start_time = utt['start_ms'] / 1000\n",
    "    end_time = utt['end_ms'] / 1000\n",
    "    print(f\"\\n[{i}] {start_time:.1f}s - {end_time:.1f}s\")\n",
    "    print(f\"    {utt['text']}\")\n",
    "    if utt.get('confidence'):\n",
    "        print(f\"    Confidence: {utt['confidence']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ All done! Download your files from /kaggle/working/output/\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional_cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CELL 10: Optional - Clean Up Large Files\n",
    "# ====================================================================\n",
    "# Uncomment if you want to delete video/audio to save space\n",
    "\n",
    "# import os\n",
    "# if os.path.exists(video_path):\n",
    "#     os.remove(video_path)\n",
    "#     print(f\"üóëÔ∏è  Deleted video: {video_path}\")\n",
    "# if os.path.exists(audio_path):\n",
    "#     os.remove(audio_path)\n",
    "#     print(f\"üóëÔ∏è  Deleted audio: {audio_path}\")\n",
    "\n",
    "print(\"üí° Tip: Keep video/audio files if you need them for later processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
